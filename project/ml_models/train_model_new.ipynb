{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c11dc8",
   "metadata": {},
   "source": [
    "# new model training for cite Guage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2456a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MEMORY-EFFICIENT DBLP CITATION NETWORK TRAINING PIPELINE\n",
      "============================================================\n",
      "Initial memory usage: 210.8MB\n",
      "Configuration loaded. Data directory: .\n",
      "Memory limit set to: 6144MB\n"
     ]
    }
   ],
   "source": [
    "# Memory-Efficient DBLP Citation Network Training Pipeline\n",
    "# Optimized for laptop/limited memory environments\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 1: Setup and Configuration\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import psutil\n",
    "import warnings\n",
    "from typing import List, Union, Optional, Dict, Any\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Memory monitoring function\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "# Enhanced logging configuration\n",
    "def setup_logging(step_name: str, log_level: int = logging.INFO) -> logging.Logger:\n",
    "    \"\"\"Setup logging with memory monitoring\"\"\"\n",
    "    log_filename = f\"{step_name}.log\"\n",
    "    \n",
    "    # Clear existing handlers to prevent duplicates\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - [MEM: {:.1f}MB] - %(message)s'.format(get_memory_usage())\n",
    "    )\n",
    "    \n",
    "    # Setup handlers\n",
    "    file_handler = logging.FileHandler(log_filename, mode='w')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Configure root logger\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        handlers=[file_handler, console_handler],\n",
    "        format='%(asctime)s - %(levelname)s - [MEM: {:.1f}MB] - %(message)s'.format(get_memory_usage())\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(step_name)\n",
    "    return logger\n",
    "\n",
    "# Memory management utilities\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Force garbage collection and log memory usage\"\"\"\n",
    "    before_mem = get_memory_usage()\n",
    "    gc.collect()\n",
    "    after_mem = get_memory_usage()\n",
    "    print(f\"Memory: {before_mem:.1f}MB -> {after_mem:.1f}MB (freed: {before_mem-after_mem:.1f}MB)\")\n",
    "\n",
    "def check_memory_limit(limit_mb: int = 8192):\n",
    "    \"\"\"Check if memory usage exceeds limit\"\"\"\n",
    "    current_mem = get_memory_usage()\n",
    "    if current_mem > limit_mb:\n",
    "        print(f\"WARNING: Memory usage ({current_mem:.1f}MB) exceeds limit ({limit_mb}MB)\")\n",
    "        force_garbage_collection()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'DATA_DIR': Path('.'),\n",
    "    'KAGGLE_CSV_FILE': 'dblp-citation-network-v14.csv',\n",
    "    \n",
    "    # --- Intermediate file paths ---\n",
    "    'CLEANED_PARQUET_FILE': 'dblp_cleaned.parquet',\n",
    "    'TFIDF_COMBINED_FEATURES_PATH': Path('.') / 'tfidf_combined_sparse_features.npz',\n",
    "    \n",
    "    # --- OUTPUT PATHS (Point to current directory as it's inside ml_models) ---\n",
    "    'ML_MODELS_DIR': Path('.'),\n",
    "    'MODEL_PATH': Path('.') / 'ridge_model.pkl',\n",
    "    'VECTORIZERS_PATH': Path('.') / 'tfidf_vectorizers.pkl',\n",
    "    'LDA_MODEL_PATH': Path('.') / 'lda_model.pkl',\n",
    "    'LDA_VECTORIZER_PATH': Path('.') / 'lda_count_vectorizer.pkl',\n",
    "    'MODEL_RESULTS_PATH': Path('.') / 'model_results.pkl',\n",
    "    \n",
    "    # Memory optimization settings\n",
    "    'CHUNK_SIZE': 10000,  # Process data in chunks\n",
    "    'MAX_MEMORY_MB': 6144,  # 6GB memory limit\n",
    "    'DTYPE_OPTIMIZATIONS': {\n",
    "        'year': 'int16',\n",
    "        'n_citation': 'int32',\n",
    "        'has_doi': 'int8',\n",
    "        'has_url': 'int8',\n",
    "        'title_length': 'int16',\n",
    "        'abstract_length': 'int32',\n",
    "        'num_keywords': 'int16',\n",
    "        'age': 'int16',\n",
    "        'num_references': 'int16'\n",
    "    },\n",
    "    \n",
    "    # TF-IDF parameters (reduced for memory efficiency)\n",
    "    'TFIDF_MAX_FEATURES': 5000,  # Reduced from 10000\n",
    "    'TFIDF_MIN_DF_TITLE_ABSTRACT': 10,  # Increased to reduce features\n",
    "    'TFIDF_MIN_DF_KEYWORDS': 2,\n",
    "    'TFIDF_NGRAM_RANGE': (1, 2),\n",
    "    \n",
    "    # LDA (Topic Modeling) parameters\n",
    "    'LDA_N_TOPICS': 10,        # Number of topics to discover\n",
    "    'LDA_MAX_FEATURES': 5000,  # Max words for the LDA vocabulary\n",
    "    'LDA_MAX_DF': 0.9,         # Ignore words appearing in > 90% of abstracts\n",
    "    'LDA_MIN_DF': 20,          # Ignore words appearing in < 20 abstracts\n",
    "    \n",
    "    # Processing settings\n",
    "    'RANDOM_STATE': 42,\n",
    "    'N_JOBS': -1  # Use all available cores\n",
    "}\n",
    "\n",
    "# Numerical features list\n",
    "NUMERICAL_FEATURES_COLS = [\n",
    "    'title_length', 'abstract_length', 'num_keywords',\n",
    "    'age', 'num_references', 'has_doi', 'has_url'\n",
    "]\n",
    "\n",
    "# Create data directory\n",
    "CONFIG['ML_MODELS_DIR'].mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MEMORY-EFFICIENT DBLP CITATION NETWORK TRAINING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Initial memory usage: {get_memory_usage():.1f}MB\")\n",
    "print(f\"Configuration loaded. Data directory: {CONFIG['DATA_DIR']}\")\n",
    "print(f\"Memory limit set to: {CONFIG['MAX_MEMORY_MB']}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545690dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:54:46,906 - INFO - [MEM: 211.2MB] - Starting efficient data loading\n",
      "2025-08-28 03:54:46,907 - INFO - [MEM: 211.2MB] - Loading data from: dblp-citation-network-v14.csv\n",
      "2025-08-28 03:54:46,907 - INFO - [MEM: 211.2MB] - File size: 3128.0MB\n",
      "2025-08-28 03:54:46,907 - INFO - [MEM: 211.2MB] - CSV field size limit increased for large fields\n",
      "2025-08-28 03:54:47,048 - INFO - [MEM: 211.2MB] - Processing chunk 1, rows: 8420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LOADING DATA\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:54:47,164 - INFO - [MEM: 211.2MB] - Processing chunk 2, rows: 9422\n",
      "2025-08-28 03:54:47,299 - INFO - [MEM: 211.2MB] - Processing chunk 3, rows: 9483\n",
      "2025-08-28 03:54:47,432 - INFO - [MEM: 211.2MB] - Processing chunk 4, rows: 9249\n",
      "2025-08-28 03:54:47,556 - INFO - [MEM: 211.2MB] - Processing chunk 5, rows: 9229\n",
      "2025-08-28 03:54:47,681 - INFO - [MEM: 211.2MB] - Processing chunk 6, rows: 9220\n",
      "2025-08-28 03:54:47,805 - INFO - [MEM: 211.2MB] - Processing chunk 7, rows: 9136\n",
      "2025-08-28 03:54:47,937 - INFO - [MEM: 211.2MB] - Processing chunk 8, rows: 9428\n",
      "2025-08-28 03:54:48,091 - INFO - [MEM: 211.2MB] - Processing chunk 9, rows: 9508\n",
      "2025-08-28 03:54:48,272 - INFO - [MEM: 211.2MB] - Processing chunk 10, rows: 9673\n",
      "2025-08-28 03:54:48,451 - INFO - [MEM: 211.2MB] - Processing chunk 11, rows: 9703\n",
      "2025-08-28 03:54:48,634 - INFO - [MEM: 211.2MB] - Processing chunk 12, rows: 9737\n",
      "2025-08-28 03:54:48,823 - INFO - [MEM: 211.2MB] - Processing chunk 13, rows: 9729\n",
      "2025-08-28 03:54:49,005 - INFO - [MEM: 211.2MB] - Processing chunk 14, rows: 9734\n",
      "2025-08-28 03:54:49,187 - INFO - [MEM: 211.2MB] - Processing chunk 15, rows: 9701\n",
      "2025-08-28 03:54:49,371 - INFO - [MEM: 211.2MB] - Processing chunk 16, rows: 9633\n",
      "2025-08-28 03:54:49,550 - INFO - [MEM: 211.2MB] - Processing chunk 17, rows: 9708\n",
      "2025-08-28 03:54:49,728 - INFO - [MEM: 211.2MB] - Processing chunk 18, rows: 9682\n",
      "2025-08-28 03:54:49,906 - INFO - [MEM: 211.2MB] - Processing chunk 19, rows: 9712\n",
      "2025-08-28 03:54:50,070 - INFO - [MEM: 211.2MB] - Processing chunk 20, rows: 9621\n",
      "2025-08-28 03:54:50,260 - INFO - [MEM: 211.2MB] - Processing chunk 21, rows: 9491\n",
      "2025-08-28 03:54:50,404 - INFO - [MEM: 211.2MB] - Processing chunk 22, rows: 9398\n",
      "2025-08-28 03:54:50,552 - INFO - [MEM: 211.2MB] - Processing chunk 23, rows: 9507\n",
      "2025-08-28 03:54:50,705 - INFO - [MEM: 211.2MB] - Processing chunk 24, rows: 9510\n",
      "2025-08-28 03:54:50,862 - INFO - [MEM: 211.2MB] - Processing chunk 25, rows: 9560\n",
      "2025-08-28 03:54:51,009 - INFO - [MEM: 211.2MB] - Processing chunk 26, rows: 9528\n",
      "2025-08-28 03:54:51,163 - INFO - [MEM: 211.2MB] - Processing chunk 27, rows: 9608\n",
      "2025-08-28 03:54:51,303 - INFO - [MEM: 211.2MB] - Processing chunk 28, rows: 9387\n",
      "2025-08-28 03:54:51,457 - INFO - [MEM: 211.2MB] - Processing chunk 29, rows: 9456\n",
      "2025-08-28 03:54:51,597 - INFO - [MEM: 211.2MB] - Processing chunk 30, rows: 9360\n",
      "2025-08-28 03:54:51,746 - INFO - [MEM: 211.2MB] - Processing chunk 31, rows: 9340\n",
      "2025-08-28 03:54:51,919 - INFO - [MEM: 211.2MB] - Processing chunk 32, rows: 9709\n",
      "2025-08-28 03:54:52,095 - INFO - [MEM: 211.2MB] - Processing chunk 33, rows: 9687\n",
      "2025-08-28 03:54:52,271 - INFO - [MEM: 211.2MB] - Processing chunk 34, rows: 9666\n",
      "2025-08-28 03:54:52,449 - INFO - [MEM: 211.2MB] - Processing chunk 35, rows: 9686\n",
      "2025-08-28 03:54:52,626 - INFO - [MEM: 211.2MB] - Processing chunk 36, rows: 9701\n",
      "2025-08-28 03:54:52,798 - INFO - [MEM: 211.2MB] - Processing chunk 37, rows: 9658\n",
      "2025-08-28 03:54:52,970 - INFO - [MEM: 211.2MB] - Processing chunk 38, rows: 9592\n",
      "2025-08-28 03:54:53,151 - INFO - [MEM: 211.2MB] - Processing chunk 39, rows: 9638\n",
      "2025-08-28 03:54:53,325 - INFO - [MEM: 211.2MB] - Processing chunk 40, rows: 9698\n",
      "2025-08-28 03:54:53,501 - INFO - [MEM: 211.2MB] - Processing chunk 41, rows: 9736\n",
      "2025-08-28 03:54:53,678 - INFO - [MEM: 211.2MB] - Processing chunk 42, rows: 9797\n",
      "2025-08-28 03:54:53,856 - INFO - [MEM: 211.2MB] - Processing chunk 43, rows: 9723\n",
      "2025-08-28 03:54:54,032 - INFO - [MEM: 211.2MB] - Processing chunk 44, rows: 9698\n",
      "2025-08-28 03:54:54,209 - INFO - [MEM: 211.2MB] - Processing chunk 45, rows: 9684\n",
      "2025-08-28 03:54:54,387 - INFO - [MEM: 211.2MB] - Processing chunk 46, rows: 9726\n",
      "2025-08-28 03:54:54,568 - INFO - [MEM: 211.2MB] - Processing chunk 47, rows: 9753\n",
      "2025-08-28 03:54:54,748 - INFO - [MEM: 211.2MB] - Processing chunk 48, rows: 9741\n",
      "2025-08-28 03:54:54,932 - INFO - [MEM: 211.2MB] - Processing chunk 49, rows: 9723\n",
      "2025-08-28 03:54:55,112 - INFO - [MEM: 211.2MB] - Processing chunk 50, rows: 9696\n",
      "2025-08-28 03:54:55,292 - INFO - [MEM: 211.2MB] - Processing chunk 51, rows: 9617\n",
      "2025-08-28 03:54:55,471 - INFO - [MEM: 211.2MB] - Processing chunk 52, rows: 9592\n",
      "2025-08-28 03:54:55,649 - INFO - [MEM: 211.2MB] - Processing chunk 53, rows: 9580\n",
      "2025-08-28 03:54:55,830 - INFO - [MEM: 211.2MB] - Processing chunk 54, rows: 9619\n",
      "2025-08-28 03:54:56,010 - INFO - [MEM: 211.2MB] - Processing chunk 55, rows: 9613\n",
      "2025-08-28 03:54:56,194 - INFO - [MEM: 211.2MB] - Processing chunk 56, rows: 9650\n",
      "2025-08-28 03:54:56,417 - INFO - [MEM: 211.2MB] - Processing chunk 57, rows: 9556\n",
      "2025-08-28 03:54:56,592 - INFO - [MEM: 211.2MB] - Processing chunk 58, rows: 9554\n",
      "2025-08-28 03:54:56,770 - INFO - [MEM: 211.2MB] - Processing chunk 59, rows: 9462\n",
      "2025-08-28 03:54:56,949 - INFO - [MEM: 211.2MB] - Processing chunk 60, rows: 9479\n",
      "2025-08-28 03:54:57,131 - INFO - [MEM: 211.2MB] - Processing chunk 61, rows: 9450\n",
      "2025-08-28 03:54:57,312 - INFO - [MEM: 211.2MB] - Processing chunk 62, rows: 9498\n",
      "2025-08-28 03:54:57,494 - INFO - [MEM: 211.2MB] - Processing chunk 63, rows: 9589\n",
      "2025-08-28 03:54:57,669 - INFO - [MEM: 211.2MB] - Processing chunk 64, rows: 9590\n",
      "2025-08-28 03:54:57,848 - INFO - [MEM: 211.2MB] - Processing chunk 65, rows: 9561\n",
      "2025-08-28 03:54:58,026 - INFO - [MEM: 211.2MB] - Processing chunk 66, rows: 9491\n",
      "2025-08-28 03:54:58,206 - INFO - [MEM: 211.2MB] - Processing chunk 67, rows: 9546\n",
      "2025-08-28 03:54:58,380 - INFO - [MEM: 211.2MB] - Processing chunk 68, rows: 9481\n",
      "2025-08-28 03:54:58,555 - INFO - [MEM: 211.2MB] - Processing chunk 69, rows: 9417\n",
      "2025-08-28 03:54:58,729 - INFO - [MEM: 211.2MB] - Processing chunk 70, rows: 9533\n",
      "2025-08-28 03:54:58,908 - INFO - [MEM: 211.2MB] - Processing chunk 71, rows: 9549\n",
      "2025-08-28 03:54:59,087 - INFO - [MEM: 211.2MB] - Processing chunk 72, rows: 9582\n",
      "2025-08-28 03:54:59,265 - INFO - [MEM: 211.2MB] - Processing chunk 73, rows: 9526\n",
      "2025-08-28 03:54:59,443 - INFO - [MEM: 211.2MB] - Processing chunk 74, rows: 9555\n",
      "2025-08-28 03:54:59,621 - INFO - [MEM: 211.2MB] - Processing chunk 75, rows: 9504\n",
      "2025-08-28 03:54:59,800 - INFO - [MEM: 211.2MB] - Processing chunk 76, rows: 9447\n",
      "2025-08-28 03:54:59,979 - INFO - [MEM: 211.2MB] - Processing chunk 77, rows: 9579\n",
      "2025-08-28 03:55:00,159 - INFO - [MEM: 211.2MB] - Processing chunk 78, rows: 9552\n",
      "2025-08-28 03:55:00,337 - INFO - [MEM: 211.2MB] - Processing chunk 79, rows: 9535\n",
      "2025-08-28 03:55:00,514 - INFO - [MEM: 211.2MB] - Processing chunk 80, rows: 9565\n",
      "2025-08-28 03:55:00,693 - INFO - [MEM: 211.2MB] - Processing chunk 81, rows: 9540\n",
      "2025-08-28 03:55:00,878 - INFO - [MEM: 211.2MB] - Processing chunk 82, rows: 9589\n",
      "2025-08-28 03:55:01,058 - INFO - [MEM: 211.2MB] - Processing chunk 83, rows: 9550\n",
      "2025-08-28 03:55:01,241 - INFO - [MEM: 211.2MB] - Processing chunk 84, rows: 9546\n",
      "2025-08-28 03:55:01,425 - INFO - [MEM: 211.2MB] - Processing chunk 85, rows: 9567\n",
      "2025-08-28 03:55:01,609 - INFO - [MEM: 211.2MB] - Processing chunk 86, rows: 9548\n",
      "2025-08-28 03:55:01,789 - INFO - [MEM: 211.2MB] - Processing chunk 87, rows: 9485\n",
      "2025-08-28 03:55:01,975 - INFO - [MEM: 211.2MB] - Processing chunk 88, rows: 9589\n",
      "2025-08-28 03:55:02,158 - INFO - [MEM: 211.2MB] - Processing chunk 89, rows: 9458\n",
      "2025-08-28 03:55:02,343 - INFO - [MEM: 211.2MB] - Processing chunk 90, rows: 9503\n",
      "2025-08-28 03:55:02,542 - INFO - [MEM: 211.2MB] - Processing chunk 91, rows: 9615\n",
      "2025-08-28 03:55:02,765 - INFO - [MEM: 211.2MB] - Processing chunk 92, rows: 9537\n",
      "2025-08-28 03:55:02,998 - INFO - [MEM: 211.2MB] - Processing chunk 93, rows: 9489\n",
      "2025-08-28 03:55:03,195 - INFO - [MEM: 211.2MB] - Processing chunk 94, rows: 9513\n",
      "2025-08-28 03:55:03,400 - INFO - [MEM: 211.2MB] - Processing chunk 95, rows: 9496\n",
      "2025-08-28 03:55:03,683 - INFO - [MEM: 211.2MB] - Processing chunk 96, rows: 9527\n",
      "2025-08-28 03:55:03,866 - INFO - [MEM: 211.2MB] - Processing chunk 97, rows: 9524\n",
      "2025-08-28 03:55:04,050 - INFO - [MEM: 211.2MB] - Processing chunk 98, rows: 9552\n",
      "2025-08-28 03:55:04,235 - INFO - [MEM: 211.2MB] - Processing chunk 99, rows: 9532\n",
      "2025-08-28 03:55:04,427 - INFO - [MEM: 211.2MB] - Processing chunk 100, rows: 9499\n",
      "2025-08-28 03:55:04,616 - INFO - [MEM: 211.2MB] - Processing chunk 101, rows: 9537\n",
      "2025-08-28 03:55:04,808 - INFO - [MEM: 211.2MB] - Processing chunk 102, rows: 9570\n",
      "2025-08-28 03:55:04,998 - INFO - [MEM: 211.2MB] - Processing chunk 103, rows: 9489\n",
      "2025-08-28 03:55:05,185 - INFO - [MEM: 211.2MB] - Processing chunk 104, rows: 9548\n",
      "2025-08-28 03:55:05,377 - INFO - [MEM: 211.2MB] - Processing chunk 105, rows: 9543\n",
      "2025-08-28 03:55:05,558 - INFO - [MEM: 211.2MB] - Processing chunk 106, rows: 9449\n",
      "2025-08-28 03:55:05,743 - INFO - [MEM: 211.2MB] - Processing chunk 107, rows: 9491\n",
      "2025-08-28 03:55:05,941 - INFO - [MEM: 211.2MB] - Processing chunk 108, rows: 9523\n",
      "2025-08-28 03:55:06,137 - INFO - [MEM: 211.2MB] - Processing chunk 109, rows: 9433\n",
      "2025-08-28 03:55:06,316 - INFO - [MEM: 211.2MB] - Processing chunk 110, rows: 9604\n",
      "2025-08-28 03:55:06,504 - INFO - [MEM: 211.2MB] - Processing chunk 111, rows: 9621\n",
      "2025-08-28 03:55:06,688 - INFO - [MEM: 211.2MB] - Processing chunk 112, rows: 9424\n",
      "2025-08-28 03:55:06,872 - INFO - [MEM: 211.2MB] - Processing chunk 113, rows: 9525\n",
      "2025-08-28 03:55:07,053 - INFO - [MEM: 211.2MB] - Processing chunk 114, rows: 9460\n",
      "2025-08-28 03:55:07,241 - INFO - [MEM: 211.2MB] - Processing chunk 115, rows: 9527\n",
      "2025-08-28 03:55:07,421 - INFO - [MEM: 211.2MB] - Processing chunk 116, rows: 9483\n",
      "2025-08-28 03:55:07,599 - INFO - [MEM: 211.2MB] - Processing chunk 117, rows: 9421\n",
      "2025-08-28 03:55:07,782 - INFO - [MEM: 211.2MB] - Processing chunk 118, rows: 9549\n",
      "2025-08-28 03:55:07,959 - INFO - [MEM: 211.2MB] - Processing chunk 119, rows: 9465\n",
      "2025-08-28 03:55:08,142 - INFO - [MEM: 211.2MB] - Processing chunk 120, rows: 9547\n",
      "2025-08-28 03:55:08,328 - INFO - [MEM: 211.2MB] - Processing chunk 121, rows: 9442\n",
      "2025-08-28 03:55:08,518 - INFO - [MEM: 211.2MB] - Processing chunk 122, rows: 9517\n",
      "2025-08-28 03:55:08,704 - INFO - [MEM: 211.2MB] - Processing chunk 123, rows: 9526\n",
      "2025-08-28 03:55:08,885 - INFO - [MEM: 211.2MB] - Processing chunk 124, rows: 9420\n",
      "2025-08-28 03:55:09,069 - INFO - [MEM: 211.2MB] - Processing chunk 125, rows: 9521\n",
      "2025-08-28 03:55:09,247 - INFO - [MEM: 211.2MB] - Processing chunk 126, rows: 9493\n",
      "2025-08-28 03:55:09,426 - INFO - [MEM: 211.2MB] - Processing chunk 127, rows: 9510\n",
      "2025-08-28 03:55:09,609 - INFO - [MEM: 211.2MB] - Processing chunk 128, rows: 9570\n",
      "2025-08-28 03:55:09,789 - INFO - [MEM: 211.2MB] - Processing chunk 129, rows: 9576\n",
      "2025-08-28 03:55:09,973 - INFO - [MEM: 211.2MB] - Processing chunk 130, rows: 9532\n",
      "2025-08-28 03:55:10,156 - INFO - [MEM: 211.2MB] - Processing chunk 131, rows: 9586\n",
      "2025-08-28 03:55:10,343 - INFO - [MEM: 211.2MB] - Processing chunk 132, rows: 9553\n",
      "2025-08-28 03:55:10,524 - INFO - [MEM: 211.2MB] - Processing chunk 133, rows: 9506\n",
      "2025-08-28 03:55:10,712 - INFO - [MEM: 211.2MB] - Processing chunk 134, rows: 9522\n",
      "2025-08-28 03:55:10,934 - INFO - [MEM: 211.2MB] - Processing chunk 135, rows: 9500\n",
      "2025-08-28 03:55:11,119 - INFO - [MEM: 211.2MB] - Processing chunk 136, rows: 9576\n",
      "2025-08-28 03:55:11,295 - INFO - [MEM: 211.2MB] - Processing chunk 137, rows: 9557\n",
      "2025-08-28 03:55:11,465 - INFO - [MEM: 211.2MB] - Processing chunk 138, rows: 9395\n",
      "2025-08-28 03:55:11,636 - INFO - [MEM: 211.2MB] - Processing chunk 139, rows: 9456\n",
      "2025-08-28 03:55:11,812 - INFO - [MEM: 211.2MB] - Processing chunk 140, rows: 9512\n",
      "2025-08-28 03:55:11,985 - INFO - [MEM: 211.2MB] - Processing chunk 141, rows: 9548\n",
      "2025-08-28 03:55:12,163 - INFO - [MEM: 211.2MB] - Processing chunk 142, rows: 9461\n",
      "2025-08-28 03:55:12,337 - INFO - [MEM: 211.2MB] - Processing chunk 143, rows: 9548\n",
      "2025-08-28 03:55:12,515 - INFO - [MEM: 211.2MB] - Processing chunk 144, rows: 9541\n",
      "2025-08-28 03:55:12,705 - INFO - [MEM: 211.2MB] - Processing chunk 145, rows: 9589\n",
      "2025-08-28 03:55:12,885 - INFO - [MEM: 211.2MB] - Processing chunk 146, rows: 9525\n",
      "2025-08-28 03:55:13,058 - INFO - [MEM: 211.2MB] - Processing chunk 147, rows: 9469\n",
      "2025-08-28 03:55:13,235 - INFO - [MEM: 211.2MB] - Processing chunk 148, rows: 9517\n",
      "2025-08-28 03:55:13,417 - INFO - [MEM: 211.2MB] - Processing chunk 149, rows: 9519\n",
      "2025-08-28 03:55:13,607 - INFO - [MEM: 211.2MB] - Processing chunk 150, rows: 9508\n",
      "2025-08-28 03:55:13,787 - INFO - [MEM: 211.2MB] - Processing chunk 151, rows: 9406\n",
      "2025-08-28 03:55:13,975 - INFO - [MEM: 211.2MB] - Processing chunk 152, rows: 9553\n",
      "2025-08-28 03:55:14,179 - INFO - [MEM: 211.2MB] - Processing chunk 153, rows: 9556\n",
      "2025-08-28 03:55:14,357 - INFO - [MEM: 211.2MB] - Processing chunk 154, rows: 9516\n",
      "2025-08-28 03:55:14,535 - INFO - [MEM: 211.2MB] - Processing chunk 155, rows: 9509\n",
      "2025-08-28 03:55:14,709 - INFO - [MEM: 211.2MB] - Processing chunk 156, rows: 9484\n",
      "2025-08-28 03:55:14,888 - INFO - [MEM: 211.2MB] - Processing chunk 157, rows: 9589\n",
      "2025-08-28 03:55:15,070 - INFO - [MEM: 211.2MB] - Processing chunk 158, rows: 9586\n",
      "2025-08-28 03:55:15,242 - INFO - [MEM: 211.2MB] - Processing chunk 159, rows: 9470\n",
      "2025-08-28 03:55:15,419 - INFO - [MEM: 211.2MB] - Processing chunk 160, rows: 9527\n",
      "2025-08-28 03:55:15,596 - INFO - [MEM: 211.2MB] - Processing chunk 161, rows: 9523\n",
      "2025-08-28 03:55:15,775 - INFO - [MEM: 211.2MB] - Processing chunk 162, rows: 9535\n",
      "2025-08-28 03:55:15,950 - INFO - [MEM: 211.2MB] - Processing chunk 163, rows: 9450\n",
      "2025-08-28 03:55:16,130 - INFO - [MEM: 211.2MB] - Processing chunk 164, rows: 9523\n",
      "2025-08-28 03:55:16,317 - INFO - [MEM: 211.2MB] - Processing chunk 165, rows: 9541\n",
      "2025-08-28 03:55:16,505 - INFO - [MEM: 211.2MB] - Processing chunk 166, rows: 9456\n",
      "2025-08-28 03:55:16,686 - INFO - [MEM: 211.2MB] - Processing chunk 167, rows: 9550\n",
      "2025-08-28 03:55:16,874 - INFO - [MEM: 211.2MB] - Processing chunk 168, rows: 9522\n",
      "2025-08-28 03:55:17,057 - INFO - [MEM: 211.2MB] - Processing chunk 169, rows: 9470\n",
      "2025-08-28 03:55:17,240 - INFO - [MEM: 211.2MB] - Processing chunk 170, rows: 9507\n",
      "2025-08-28 03:55:17,434 - INFO - [MEM: 211.2MB] - Processing chunk 171, rows: 9471\n",
      "2025-08-28 03:55:17,622 - INFO - [MEM: 211.2MB] - Processing chunk 172, rows: 9468\n",
      "2025-08-28 03:55:17,802 - INFO - [MEM: 211.2MB] - Processing chunk 173, rows: 9456\n",
      "2025-08-28 03:55:18,037 - INFO - [MEM: 211.2MB] - Processing chunk 174, rows: 9550\n",
      "2025-08-28 03:55:18,225 - INFO - [MEM: 211.2MB] - Processing chunk 175, rows: 9534\n",
      "2025-08-28 03:55:18,399 - INFO - [MEM: 211.2MB] - Processing chunk 176, rows: 9419\n",
      "2025-08-28 03:55:18,584 - INFO - [MEM: 211.2MB] - Processing chunk 177, rows: 9482\n",
      "2025-08-28 03:55:18,769 - INFO - [MEM: 211.2MB] - Processing chunk 178, rows: 9553\n",
      "2025-08-28 03:55:18,956 - INFO - [MEM: 211.2MB] - Processing chunk 179, rows: 9499\n",
      "2025-08-28 03:55:19,138 - INFO - [MEM: 211.2MB] - Processing chunk 180, rows: 9590\n",
      "2025-08-28 03:55:19,325 - INFO - [MEM: 211.2MB] - Processing chunk 181, rows: 9556\n",
      "2025-08-28 03:55:19,504 - INFO - [MEM: 211.2MB] - Processing chunk 182, rows: 9512\n",
      "2025-08-28 03:55:19,682 - INFO - [MEM: 211.2MB] - Processing chunk 183, rows: 9527\n",
      "2025-08-28 03:55:19,858 - INFO - [MEM: 211.2MB] - Processing chunk 184, rows: 9468\n",
      "2025-08-28 03:55:20,034 - INFO - [MEM: 211.2MB] - Processing chunk 185, rows: 9430\n",
      "2025-08-28 03:55:20,213 - INFO - [MEM: 211.2MB] - Processing chunk 186, rows: 9405\n",
      "2025-08-28 03:55:20,392 - INFO - [MEM: 211.2MB] - Processing chunk 187, rows: 9479\n",
      "2025-08-28 03:55:20,573 - INFO - [MEM: 211.2MB] - Processing chunk 188, rows: 9589\n",
      "2025-08-28 03:55:20,756 - INFO - [MEM: 211.2MB] - Processing chunk 189, rows: 9478\n",
      "2025-08-28 03:55:20,934 - INFO - [MEM: 211.2MB] - Processing chunk 190, rows: 9454\n",
      "2025-08-28 03:55:21,122 - INFO - [MEM: 211.2MB] - Processing chunk 191, rows: 9504\n",
      "2025-08-28 03:55:21,349 - INFO - [MEM: 211.2MB] - Processing chunk 192, rows: 9568\n",
      "2025-08-28 03:55:21,532 - INFO - [MEM: 211.2MB] - Processing chunk 193, rows: 9461\n",
      "2025-08-28 03:55:21,720 - INFO - [MEM: 211.2MB] - Processing chunk 194, rows: 9522\n",
      "2025-08-28 03:55:21,907 - INFO - [MEM: 211.2MB] - Processing chunk 195, rows: 9504\n",
      "2025-08-28 03:55:22,096 - INFO - [MEM: 211.2MB] - Processing chunk 196, rows: 9536\n",
      "2025-08-28 03:55:22,282 - INFO - [MEM: 211.2MB] - Processing chunk 197, rows: 9540\n",
      "2025-08-28 03:55:22,465 - INFO - [MEM: 211.2MB] - Processing chunk 198, rows: 9574\n",
      "2025-08-28 03:55:22,647 - INFO - [MEM: 211.2MB] - Processing chunk 199, rows: 9473\n",
      "2025-08-28 03:55:22,832 - INFO - [MEM: 211.2MB] - Processing chunk 200, rows: 9552\n",
      "2025-08-28 03:55:23,017 - INFO - [MEM: 211.2MB] - Processing chunk 201, rows: 9561\n",
      "2025-08-28 03:55:23,207 - INFO - [MEM: 211.2MB] - Processing chunk 202, rows: 9547\n",
      "2025-08-28 03:55:23,392 - INFO - [MEM: 211.2MB] - Processing chunk 203, rows: 9591\n",
      "2025-08-28 03:55:23,575 - INFO - [MEM: 211.2MB] - Processing chunk 204, rows: 9477\n",
      "2025-08-28 03:55:23,759 - INFO - [MEM: 211.2MB] - Processing chunk 205, rows: 9472\n",
      "2025-08-28 03:55:23,941 - INFO - [MEM: 211.2MB] - Processing chunk 206, rows: 9534\n",
      "2025-08-28 03:55:24,125 - INFO - [MEM: 211.2MB] - Processing chunk 207, rows: 9552\n",
      "2025-08-28 03:55:24,307 - INFO - [MEM: 211.2MB] - Processing chunk 208, rows: 9506\n",
      "2025-08-28 03:55:24,489 - INFO - [MEM: 211.2MB] - Processing chunk 209, rows: 9518\n",
      "2025-08-28 03:55:24,678 - INFO - [MEM: 211.2MB] - Processing chunk 210, rows: 9520\n",
      "2025-08-28 03:55:24,867 - INFO - [MEM: 211.2MB] - Processing chunk 211, rows: 9504\n",
      "2025-08-28 03:55:25,048 - INFO - [MEM: 211.2MB] - Processing chunk 212, rows: 9476\n",
      "2025-08-28 03:55:25,265 - INFO - [MEM: 211.2MB] - Processing chunk 213, rows: 7229\n",
      "2025-08-28 03:55:25,271 - INFO - [MEM: 211.2MB] - Loaded 213 chunks with total 2027734 rows\n",
      "2025-08-28 03:55:25,272 - INFO - [MEM: 211.2MB] - Combining chunks...\n",
      "2025-08-28 03:55:26,692 - INFO - [MEM: 211.2MB] - Final dataset shape: (2027734, 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 4793.5MB -> 4802.1MB (freed: -8.6MB)\n",
      "Data loaded successfully. Shape: (2027734, 19)\n",
      "Memory usage after loading: 4802.3MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 2: Data Loading with Memory Optimization\n",
    "# ============================================================================\n",
    "\n",
    "def load_data_efficiently(file_path: Path, chunk_size: int = 10000) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV data efficiently using chunking and memory optimization\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    logger = setup_logging(\"data_loading\")\n",
    "    logger.info(\"Starting efficient data loading\")\n",
    "    \n",
    "    full_csv_path = CONFIG['DATA_DIR'] / CONFIG['KAGGLE_CSV_FILE']\n",
    "    \n",
    "    if not full_csv_path.exists():\n",
    "        error_msg = f\"Dataset CSV file not found at '{full_csv_path}'\"\n",
    "        logger.error(error_msg)\n",
    "        raise FileNotFoundError(error_msg)\n",
    "    \n",
    "    logger.info(f\"Loading data from: {full_csv_path}\")\n",
    "    \n",
    "    # First, get file size for progress tracking\n",
    "    file_size = full_csv_path.stat().st_size / 1024 / 1024  # MB\n",
    "    logger.info(f\"File size: {file_size:.1f}MB\")\n",
    "    \n",
    "    try:\n",
    "        # Increase CSV field size limit to handle large fields\n",
    "        csv.field_size_limit(min(2**31-1, 10**7))  # Set to 10MB or system max\n",
    "        logger.info(\"CSV field size limit increased for large fields\")\n",
    "        \n",
    "        # Read in chunks to manage memory\n",
    "        chunks = []\n",
    "        total_rows = 0\n",
    "        \n",
    "        chunk_reader = pd.read_csv(\n",
    "            full_csv_path,\n",
    "            sep='|',\n",
    "            engine='python',\n",
    "            on_bad_lines='skip',\n",
    "            chunksize=chunk_size,\n",
    "            low_memory=True,\n",
    "            quoting=csv.QUOTE_NONE,  # Handle quotes more flexibly\n",
    "            escapechar='\\\\',         # Handle escape characters\n",
    "            dtype=str               # Read everything as string first\n",
    "        )\n",
    "        \n",
    "        for i, chunk in enumerate(chunk_reader):\n",
    "            logger.info(f\"Processing chunk {i+1}, rows: {len(chunk)}\")\n",
    "            \n",
    "            # Basic cleaning on chunk\n",
    "            chunk = optimize_chunk_dtypes(chunk)\n",
    "            chunks.append(chunk)\n",
    "            total_rows += len(chunk)\n",
    "            \n",
    "            # Memory check\n",
    "            if check_memory_limit(CONFIG['MAX_MEMORY_MB']):\n",
    "                logger.warning(f\"Memory limit reached after {i+1} chunks\")\n",
    "                break\n",
    "        \n",
    "        logger.info(f\"Loaded {len(chunks)} chunks with total {total_rows} rows\")\n",
    "        \n",
    "        # Combine chunks efficiently\n",
    "        logger.info(\"Combining chunks...\")\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        del chunks  # Free memory immediately\n",
    "        force_garbage_collection()\n",
    "        \n",
    "        logger.info(f\"Final dataset shape: {df.shape}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def optimize_chunk_dtypes(chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Optimize data types for a chunk to reduce memory usage\"\"\"\n",
    "    # Convert numeric columns with appropriate dtypes\n",
    "    if 'year' in chunk.columns:\n",
    "        chunk['year'] = pd.to_numeric(chunk['year'], errors='coerce', downcast='integer')\n",
    "    \n",
    "    if 'n_citation' in chunk.columns:\n",
    "        chunk['n_citation'] = pd.to_numeric(chunk['n_citation'], errors='coerce', downcast='integer')\n",
    "    \n",
    "    # Convert string columns to category if they have low cardinality\n",
    "    for col in ['lang', 'doc_type']:\n",
    "        if col in chunk.columns:\n",
    "            unique_ratio = chunk[col].nunique() / len(chunk)\n",
    "            if unique_ratio < 0.5:  # If less than 50% unique values\n",
    "                chunk[col] = chunk[col].astype('category')\n",
    "    \n",
    "    return chunk\n",
    "\n",
    "# Load data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df = load_data_efficiently(CONFIG['DATA_DIR'] / CONFIG['KAGGLE_CSV_FILE'], CONFIG['CHUNK_SIZE'])\n",
    "print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "print(f\"Memory usage after loading: {get_memory_usage():.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b44729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:55:26,704 - INFO - [MEM: 4804.2MB] - Starting Memory-Efficient EDA\n",
      "2025-08-28 03:55:26,704 - INFO - [MEM: 4804.2MB] - Dataset shape: (2027734, 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXPLORATORY DATA ANALYSIS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:55:32,340 - INFO - [MEM: 4804.2MB] - Memory usage: 4560.0MB\n",
      "2025-08-28 03:55:32,363 - INFO - [MEM: 4804.2MB] - Analyzing missing values...\n",
      "2025-08-28 03:55:34,539 - INFO - [MEM: 4804.2MB] - Columns with missing values: 19\n",
      "2025-08-28 03:55:34,539 - INFO - [MEM: 4804.2MB] - Analyzing target variable (n_citation)...\n",
      "2025-08-28 03:55:34,611 - INFO - [MEM: 4804.2MB] - Citation statistics:\n",
      "count    1.732752e+06\n",
      "mean     3.947402e+01\n",
      "std      3.290488e+02\n",
      "min      0.000000e+00\n",
      "25%      2.000000e+00\n",
      "50%      7.000000e+00\n",
      "75%      2.600000e+01\n",
      "max      1.218600e+05\n",
      "Name: n_citation, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:55:34,976 - INFO - [MEM: 4804.2MB] - Analyzing publication years...\n",
      "2025-08-28 03:55:35,042 - INFO - [MEM: 4804.2MB] - Year statistics:\n",
      "count    1.732750e+06\n",
      "mean     2.004538e+03\n",
      "std      1.014931e+01\n",
      "min      0.000000e+00\n",
      "25%      2.001000e+03\n",
      "50%      2.007000e+03\n",
      "75%      2.011000e+03\n",
      "max      2.022000e+03\n",
      "Name: year, dtype: float64\n",
      "2025-08-28 03:55:35,043 - INFO - [MEM: 4804.2MB] - Analyzing text fields...\n",
      "2025-08-28 03:55:35,043 - INFO - [MEM: 4804.2MB] - Analyzing title...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 4590.1MB -> 4648.7MB (freed: -58.6MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:55:35,252 - INFO - [MEM: 4804.2MB] - title length statistics (sample):\n",
      "count    1000.000000\n",
      "mean       75.714000\n",
      "std        59.453502\n",
      "min         7.000000\n",
      "25%        51.000000\n",
      "50%        68.000000\n",
      "75%        87.000000\n",
      "max       926.000000\n",
      "Name: title, dtype: float64\n",
      "2025-08-28 03:55:35,253 - INFO - [MEM: 4804.2MB] - Analyzing abstract...\n",
      "2025-08-28 03:55:35,629 - INFO - [MEM: 4804.2MB] - abstract length statistics (sample):\n",
      "count    1000.000000\n",
      "mean      871.858000\n",
      "std       433.127937\n",
      "min         1.000000\n",
      "25%       582.750000\n",
      "50%       832.000000\n",
      "75%      1120.000000\n",
      "max      2859.000000\n",
      "Name: abstract, dtype: float64\n",
      "2025-08-28 03:55:35,629 - INFO - [MEM: 4804.2MB] - Analyzing keywords...\n",
      "2025-08-28 03:55:35,849 - INFO - [MEM: 4804.2MB] - keywords length statistics (sample):\n",
      "count    1000.000000\n",
      "mean      195.489000\n",
      "std       180.631286\n",
      "min         2.000000\n",
      "25%        44.000000\n",
      "50%       139.500000\n",
      "75%       315.000000\n",
      "max      1011.000000\n",
      "Name: keywords, dtype: float64\n",
      "2025-08-28 03:55:35,850 - INFO - [MEM: 4804.2MB] - EDA completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA completed. Memory usage: 4679.3MB\n",
      "\n",
      "Top columns with missing values:\n",
      "          Missing_Count  Missing_Percentage\n",
      "isbn            1648056           81.275749\n",
      "issue           1186867           58.531691\n",
      "issn            1027093           50.652255\n",
      "volume           940837           46.398443\n",
      "abstract         610654           30.115094\n",
      "\n",
      "Citation count statistics:\n",
      "count    1.732752e+06\n",
      "mean     3.947402e+01\n",
      "std      3.290488e+02\n",
      "min      0.000000e+00\n",
      "25%      2.000000e+00\n",
      "50%      7.000000e+00\n",
      "75%      2.600000e+01\n",
      "max      1.218600e+05\n",
      "Name: n_citation, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 3: Memory-Efficient Exploratory Data Analysis\n",
    "# ============================================================================\n",
    "\n",
    "def perform_memory_efficient_eda(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Perform EDA with memory efficiency in mind\"\"\"\n",
    "    logger = setup_logging(\"eda\")\n",
    "    logger.info(\"Starting Memory-Efficient EDA\")\n",
    "    \n",
    "    eda_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Basic info\n",
    "        logger.info(f\"Dataset shape: {df.shape}\")\n",
    "        logger.info(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f}MB\")\n",
    "        \n",
    "        # Missing values analysis\n",
    "        logger.info(\"Analyzing missing values...\")\n",
    "        missing_info = df.isnull().sum()\n",
    "        missing_pct = (missing_info / len(df)) * 100\n",
    "        \n",
    "        missing_df = pd.DataFrame({\n",
    "            'Missing_Count': missing_info,\n",
    "            'Missing_Percentage': missing_pct\n",
    "        }).sort_values('Missing_Percentage', ascending=False)\n",
    "        \n",
    "        eda_results['missing_values'] = missing_df[missing_df['Missing_Count'] > 0]\n",
    "        logger.info(f\"Columns with missing values: {len(eda_results['missing_values'])}\")\n",
    "        \n",
    "        # Target variable analysis (n_citation)\n",
    "        if 'n_citation' in df.columns:\n",
    "            logger.info(\"Analyzing target variable (n_citation)...\")\n",
    "            df['n_citation'] = pd.to_numeric(df['n_citation'], errors='coerce')\n",
    "            \n",
    "            citation_stats = df['n_citation'].describe()\n",
    "            eda_results['citation_stats'] = citation_stats\n",
    "            logger.info(f\"Citation statistics:\\n{citation_stats}\")\n",
    "            \n",
    "            # Create histogram efficiently (sample if dataset is large)\n",
    "            sample_size = min(50000, len(df))\n",
    "            sample_df = df.sample(n=sample_size, random_state=CONFIG['RANDOM_STATE'])\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(sample_df['n_citation'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "            plt.title(f'Distribution of Citation Counts (Sample: {sample_size} records)')\n",
    "            plt.xlabel('Citation Count')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.yscale('log')  # Log scale due to typical skewness\n",
    "            plt.show()\n",
    "            \n",
    "            del sample_df\n",
    "            force_garbage_collection()\n",
    "        \n",
    "        # Year analysis\n",
    "        if 'year' in df.columns:\n",
    "            logger.info(\"Analyzing publication years...\")\n",
    "            df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "            year_stats = df['year'].describe()\n",
    "            eda_results['year_stats'] = year_stats\n",
    "            logger.info(f\"Year statistics:\\n{year_stats}\")\n",
    "        \n",
    "        # Text field analysis (sample-based to save memory)\n",
    "        text_cols = ['title', 'abstract', 'keywords']\n",
    "        logger.info(\"Analyzing text fields...\")\n",
    "        \n",
    "        for col in text_cols:\n",
    "            if col in df.columns:\n",
    "                logger.info(f\"Analyzing {col}...\")\n",
    "                \n",
    "                # Sample for analysis\n",
    "                sample_texts = df[col].dropna().sample(min(1000, df[col].dropna().shape[0]), \n",
    "                                                     random_state=CONFIG['RANDOM_STATE'])\n",
    "                \n",
    "                if col == 'keywords':\n",
    "                    # Special handling for keywords\n",
    "                    sample_lengths = sample_texts.apply(lambda x: len(str(x)))\n",
    "                else:\n",
    "                    sample_lengths = sample_texts.str.len()\n",
    "                \n",
    "                eda_results[f'{col}_length_stats'] = sample_lengths.describe()\n",
    "                logger.info(f\"{col} length statistics (sample):\\n{sample_lengths.describe()}\")\n",
    "        \n",
    "        logger.info(\"EDA completed successfully\")\n",
    "        return eda_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during EDA: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Perform EDA\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "eda_results = perform_memory_efficient_eda(df)\n",
    "print(f\"EDA completed. Memory usage: {get_memory_usage():.1f}MB\")\n",
    "\n",
    "# Display key findings\n",
    "if 'missing_values' in eda_results and not eda_results['missing_values'].empty:\n",
    "    print(\"\\nTop columns with missing values:\")\n",
    "    print(eda_results['missing_values'].head())\n",
    "\n",
    "if 'citation_stats' in eda_results:\n",
    "    print(f\"\\nCitation count statistics:\")\n",
    "    print(eda_results['citation_stats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d71e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:55:35,865 - INFO - [MEM: 4680.0MB] - Starting data cleaning on 2027734 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATA CLEANING\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:55:38,741 - INFO - [MEM: 4680.0MB] - Removed 294982 rows with missing citations\n",
      "2025-08-28 03:55:38,784 - INFO - [MEM: 4680.0MB] - Filled missing years with mode: 2012\n",
      "2025-08-28 03:55:38,785 - INFO - [MEM: 4680.0MB] - Processing keywords...\n",
      "2025-08-28 03:55:38,940 - INFO - [MEM: 4680.0MB] - Processed keywords for 5000/1732752 records\n",
      "2025-08-28 03:55:39,364 - INFO - [MEM: 4680.0MB] - Processed keywords for 25000/1732752 records\n",
      "2025-08-28 03:55:39,875 - INFO - [MEM: 4680.0MB] - Processed keywords for 45000/1732752 records\n",
      "2025-08-28 03:55:40,536 - INFO - [MEM: 4680.0MB] - Processed keywords for 65000/1732752 records\n",
      "2025-08-28 03:55:41,200 - INFO - [MEM: 4680.0MB] - Processed keywords for 85000/1732752 records\n",
      "2025-08-28 03:55:41,875 - INFO - [MEM: 4680.0MB] - Processed keywords for 105000/1732752 records\n",
      "2025-08-28 03:55:42,618 - INFO - [MEM: 4680.0MB] - Processed keywords for 125000/1732752 records\n",
      "2025-08-28 03:55:43,293 - INFO - [MEM: 4680.0MB] - Processed keywords for 145000/1732752 records\n",
      "2025-08-28 03:55:43,881 - INFO - [MEM: 4680.0MB] - Processed keywords for 165000/1732752 records\n",
      "2025-08-28 03:55:44,471 - INFO - [MEM: 4680.0MB] - Processed keywords for 185000/1732752 records\n",
      "2025-08-28 03:55:45,153 - INFO - [MEM: 4680.0MB] - Processed keywords for 205000/1732752 records\n",
      "2025-08-28 03:55:45,724 - INFO - [MEM: 4680.0MB] - Processed keywords for 225000/1732752 records\n",
      "2025-08-28 03:55:46,318 - INFO - [MEM: 4680.0MB] - Processed keywords for 245000/1732752 records\n",
      "2025-08-28 03:55:46,986 - INFO - [MEM: 4680.0MB] - Processed keywords for 265000/1732752 records\n",
      "2025-08-28 03:55:47,654 - INFO - [MEM: 4680.0MB] - Processed keywords for 285000/1732752 records\n",
      "2025-08-28 03:55:48,454 - INFO - [MEM: 4680.0MB] - Processed keywords for 305000/1732752 records\n",
      "2025-08-28 03:55:49,113 - INFO - [MEM: 4680.0MB] - Processed keywords for 325000/1732752 records\n",
      "2025-08-28 03:55:49,768 - INFO - [MEM: 4680.0MB] - Processed keywords for 345000/1732752 records\n",
      "2025-08-28 03:55:50,423 - INFO - [MEM: 4680.0MB] - Processed keywords for 365000/1732752 records\n",
      "2025-08-28 03:55:51,081 - INFO - [MEM: 4680.0MB] - Processed keywords for 385000/1732752 records\n",
      "2025-08-28 03:55:51,739 - INFO - [MEM: 4680.0MB] - Processed keywords for 405000/1732752 records\n",
      "2025-08-28 03:55:52,581 - INFO - [MEM: 4680.0MB] - Processed keywords for 425000/1732752 records\n",
      "2025-08-28 03:55:53,250 - INFO - [MEM: 4680.0MB] - Processed keywords for 445000/1732752 records\n",
      "2025-08-28 03:55:53,901 - INFO - [MEM: 4680.0MB] - Processed keywords for 465000/1732752 records\n",
      "2025-08-28 03:55:54,552 - INFO - [MEM: 4680.0MB] - Processed keywords for 485000/1732752 records\n",
      "2025-08-28 03:55:55,204 - INFO - [MEM: 4680.0MB] - Processed keywords for 505000/1732752 records\n",
      "2025-08-28 03:55:55,851 - INFO - [MEM: 4680.0MB] - Processed keywords for 525000/1732752 records\n",
      "2025-08-28 03:55:56,498 - INFO - [MEM: 4680.0MB] - Processed keywords for 545000/1732752 records\n",
      "2025-08-28 03:55:57,369 - INFO - [MEM: 4680.0MB] - Processed keywords for 565000/1732752 records\n",
      "2025-08-28 03:55:58,016 - INFO - [MEM: 4680.0MB] - Processed keywords for 585000/1732752 records\n",
      "2025-08-28 03:55:58,660 - INFO - [MEM: 4680.0MB] - Processed keywords for 605000/1732752 records\n",
      "2025-08-28 03:55:59,310 - INFO - [MEM: 4680.0MB] - Processed keywords for 625000/1732752 records\n",
      "2025-08-28 03:55:59,958 - INFO - [MEM: 4680.0MB] - Processed keywords for 645000/1732752 records\n",
      "2025-08-28 03:56:00,611 - INFO - [MEM: 4680.0MB] - Processed keywords for 665000/1732752 records\n",
      "2025-08-28 03:56:01,256 - INFO - [MEM: 4680.0MB] - Processed keywords for 685000/1732752 records\n",
      "2025-08-28 03:56:01,901 - INFO - [MEM: 4680.0MB] - Processed keywords for 705000/1732752 records\n",
      "2025-08-28 03:56:02,549 - INFO - [MEM: 4680.0MB] - Processed keywords for 725000/1732752 records\n",
      "2025-08-28 03:56:03,197 - INFO - [MEM: 4680.0MB] - Processed keywords for 745000/1732752 records\n",
      "2025-08-28 03:56:04,137 - INFO - [MEM: 4680.0MB] - Processed keywords for 765000/1732752 records\n",
      "2025-08-28 03:56:04,789 - INFO - [MEM: 4680.0MB] - Processed keywords for 785000/1732752 records\n",
      "2025-08-28 03:56:05,438 - INFO - [MEM: 4680.0MB] - Processed keywords for 805000/1732752 records\n",
      "2025-08-28 03:56:06,088 - INFO - [MEM: 4680.0MB] - Processed keywords for 825000/1732752 records\n",
      "2025-08-28 03:56:06,743 - INFO - [MEM: 4680.0MB] - Processed keywords for 845000/1732752 records\n",
      "2025-08-28 03:56:07,399 - INFO - [MEM: 4680.0MB] - Processed keywords for 865000/1732752 records\n",
      "2025-08-28 03:56:08,049 - INFO - [MEM: 4680.0MB] - Processed keywords for 885000/1732752 records\n",
      "2025-08-28 03:56:08,697 - INFO - [MEM: 4680.0MB] - Processed keywords for 905000/1732752 records\n",
      "2025-08-28 03:56:09,348 - INFO - [MEM: 4680.0MB] - Processed keywords for 925000/1732752 records\n",
      "2025-08-28 03:56:09,992 - INFO - [MEM: 4680.0MB] - Processed keywords for 945000/1732752 records\n",
      "2025-08-28 03:56:10,632 - INFO - [MEM: 4680.0MB] - Processed keywords for 965000/1732752 records\n",
      "2025-08-28 03:56:11,632 - INFO - [MEM: 4680.0MB] - Processed keywords for 985000/1732752 records\n",
      "2025-08-28 03:56:12,276 - INFO - [MEM: 4680.0MB] - Processed keywords for 1005000/1732752 records\n",
      "2025-08-28 03:56:12,904 - INFO - [MEM: 4680.0MB] - Processed keywords for 1025000/1732752 records\n",
      "2025-08-28 03:56:13,543 - INFO - [MEM: 4680.0MB] - Processed keywords for 1045000/1732752 records\n",
      "2025-08-28 03:56:14,187 - INFO - [MEM: 4680.0MB] - Processed keywords for 1065000/1732752 records\n",
      "2025-08-28 03:56:14,823 - INFO - [MEM: 4680.0MB] - Processed keywords for 1085000/1732752 records\n",
      "2025-08-28 03:56:15,458 - INFO - [MEM: 4680.0MB] - Processed keywords for 1105000/1732752 records\n",
      "2025-08-28 03:56:16,078 - INFO - [MEM: 4680.0MB] - Processed keywords for 1125000/1732752 records\n",
      "2025-08-28 03:56:16,688 - INFO - [MEM: 4680.0MB] - Processed keywords for 1145000/1732752 records\n",
      "2025-08-28 03:56:17,314 - INFO - [MEM: 4680.0MB] - Processed keywords for 1165000/1732752 records\n",
      "2025-08-28 03:56:17,949 - INFO - [MEM: 4680.0MB] - Processed keywords for 1185000/1732752 records\n",
      "2025-08-28 03:56:18,571 - INFO - [MEM: 4680.0MB] - Processed keywords for 1205000/1732752 records\n",
      "2025-08-28 03:56:19,200 - INFO - [MEM: 4680.0MB] - Processed keywords for 1225000/1732752 records\n",
      "2025-08-28 03:56:19,847 - INFO - [MEM: 4680.0MB] - Processed keywords for 1245000/1732752 records\n",
      "2025-08-28 03:56:20,479 - INFO - [MEM: 4680.0MB] - Processed keywords for 1265000/1732752 records\n",
      "2025-08-28 03:56:21,577 - INFO - [MEM: 4680.0MB] - Processed keywords for 1285000/1732752 records\n",
      "2025-08-28 03:56:22,206 - INFO - [MEM: 4680.0MB] - Processed keywords for 1305000/1732752 records\n",
      "2025-08-28 03:56:22,846 - INFO - [MEM: 4680.0MB] - Processed keywords for 1325000/1732752 records\n",
      "2025-08-28 03:56:23,473 - INFO - [MEM: 4680.0MB] - Processed keywords for 1345000/1732752 records\n",
      "2025-08-28 03:56:24,098 - INFO - [MEM: 4680.0MB] - Processed keywords for 1365000/1732752 records\n",
      "2025-08-28 03:56:24,728 - INFO - [MEM: 4680.0MB] - Processed keywords for 1385000/1732752 records\n",
      "2025-08-28 03:56:25,351 - INFO - [MEM: 4680.0MB] - Processed keywords for 1405000/1732752 records\n",
      "2025-08-28 03:56:25,990 - INFO - [MEM: 4680.0MB] - Processed keywords for 1425000/1732752 records\n",
      "2025-08-28 03:56:26,626 - INFO - [MEM: 4680.0MB] - Processed keywords for 1445000/1732752 records\n",
      "2025-08-28 03:56:27,284 - INFO - [MEM: 4680.0MB] - Processed keywords for 1465000/1732752 records\n",
      "2025-08-28 03:56:27,927 - INFO - [MEM: 4680.0MB] - Processed keywords for 1485000/1732752 records\n",
      "2025-08-28 03:56:28,561 - INFO - [MEM: 4680.0MB] - Processed keywords for 1505000/1732752 records\n",
      "2025-08-28 03:56:29,206 - INFO - [MEM: 4680.0MB] - Processed keywords for 1525000/1732752 records\n",
      "2025-08-28 03:56:29,850 - INFO - [MEM: 4680.0MB] - Processed keywords for 1545000/1732752 records\n",
      "2025-08-28 03:56:30,498 - INFO - [MEM: 4680.0MB] - Processed keywords for 1565000/1732752 records\n",
      "2025-08-28 03:56:31,145 - INFO - [MEM: 4680.0MB] - Processed keywords for 1585000/1732752 records\n",
      "2025-08-28 03:56:31,787 - INFO - [MEM: 4680.0MB] - Processed keywords for 1605000/1732752 records\n",
      "2025-08-28 03:56:32,430 - INFO - [MEM: 4680.0MB] - Processed keywords for 1625000/1732752 records\n",
      "2025-08-28 03:56:33,654 - INFO - [MEM: 4680.0MB] - Processed keywords for 1645000/1732752 records\n",
      "2025-08-28 03:56:34,293 - INFO - [MEM: 4680.0MB] - Processed keywords for 1665000/1732752 records\n",
      "2025-08-28 03:56:34,930 - INFO - [MEM: 4680.0MB] - Processed keywords for 1685000/1732752 records\n",
      "2025-08-28 03:56:35,563 - INFO - [MEM: 4680.0MB] - Processed keywords for 1705000/1732752 records\n",
      "2025-08-28 03:56:36,186 - INFO - [MEM: 4680.0MB] - Processed keywords for 1725000/1732752 records\n",
      "2025-08-28 03:56:36,443 - INFO - [MEM: 4680.0MB] - Keywords processing completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 5980.5MB -> 5980.5MB (freed: 0.0MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:56:38,078 - INFO - [MEM: 4680.0MB] - Cleaned text column: title\n",
      "2025-08-28 03:56:42,413 - INFO - [MEM: 4680.0MB] - Cleaned text column: abstract\n",
      "2025-08-28 03:56:43,792 - INFO - [MEM: 4680.0MB] - Cleaned text column: venue\n",
      "2025-08-28 03:56:44,347 - INFO - [MEM: 4680.0MB] - Cleaned text column: doc_type\n",
      "2025-08-28 03:56:44,347 - INFO - [MEM: 4680.0MB] - Processing references...\n",
      "2025-08-28 03:57:11,864 - INFO - [MEM: 4680.0MB] - References processed\n",
      "2025-08-28 03:57:11,910 - INFO - [MEM: 4680.0MB] - Data cleaning completed. Shape: (1732752, 19)\n",
      "2025-08-28 03:57:11,911 - INFO - [MEM: 4680.0MB] - Memory usage: 4680.0MB -> 4516.2MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed. Final shape: (1732752, 19)\n",
      "Memory usage after cleaning: 4516.7MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# BLOCK 4: Memory-Efficient Data Cleaning\n",
    "# ============================================================================\n",
    "\n",
    "import ast\n",
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "import re\n",
    "\n",
    "def robust_keyword_parser(keyword_input: Union[str, list, None]) -> List[str]:\n",
    "    \"\"\"Robustly parse keywords with memory efficiency and escape issues fixed\"\"\"\n",
    "    if pd.isna(keyword_input) or keyword_input in ['[]', '{}', 'None', 'none', 'null', '']:\n",
    "        return []\n",
    "    \n",
    "    if isinstance(keyword_input, list):\n",
    "        return clean_keyword_list(keyword_input)\n",
    "    \n",
    "    keyword_str = str(keyword_input).strip()\n",
    "\n",
    "    # Escape invalid backslashes to avoid SyntaxWarnings\n",
    "    keyword_str = re.sub(r'\\\\(?![nrt\\\\\"])', r'\\\\\\\\', keyword_str)\n",
    "\n",
    "    # Try parsing as literal\n",
    "    if keyword_str.startswith('[') and keyword_str.endswith(']'):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(keyword_str)\n",
    "            if isinstance(parsed, list):\n",
    "                return clean_keyword_list(parsed)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "    # Fallback to comma separation\n",
    "    if ',' in keyword_str:\n",
    "        keywords = [kw.strip() for kw in keyword_str.split(',')]\n",
    "        return clean_keyword_list(keywords)\n",
    "    \n",
    "    # Space separation as last resort\n",
    "    keywords = keyword_str.split()\n",
    "    return clean_keyword_list(keywords)\n",
    "\n",
    "\n",
    "def clean_keyword_list(keyword_list: List[Union[str, int, float]]) -> List[str]:\n",
    "    \"\"\"Clean individual keywords in a list\"\"\"\n",
    "    cleaned = []\n",
    "    blacklist = {'', 'none', 'null', 'na', 'n/a', 'keyword', 'keywords', 'none type'}\n",
    "    \n",
    "    for kw in keyword_list:\n",
    "        if pd.isna(kw):\n",
    "            continue\n",
    "            \n",
    "        kw_str = str(kw).strip().lower()\n",
    "        kw_str = re.sub(r'[^\\w\\s\\-]', '', kw_str)  # Escaped hyphen\n",
    "\n",
    "        \n",
    "        if (len(kw_str) > 1 and \n",
    "            not kw_str.isnumeric() and \n",
    "            kw_str not in blacklist):\n",
    "            cleaned.append(kw_str)\n",
    "    \n",
    "    return list(set(cleaned))\n",
    "\n",
    "def clean_data_efficiently(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean data with memory optimization\"\"\"\n",
    "    logger = setup_logging(\"data_cleaning\")\n",
    "    logger.info(f\"Starting data cleaning on {len(df)} records\")\n",
    "    \n",
    "    initial_memory = get_memory_usage()\n",
    "    \n",
    "    try:\n",
    "        # Drop unnecessary columns\n",
    "        # Fix for NameError: Define the list, even if empty.\n",
    "        columns_to_drop = []\n",
    "        existing_cols_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_cols_to_drop:\n",
    "            df = df.drop(columns=existing_cols_to_drop)\n",
    "            logger.info(f\"Dropped columns: {existing_cols_to_drop}\")\n",
    "            force_garbage_collection()\n",
    "        \n",
    "        # Handle target variable (n_citation)\n",
    "        if 'n_citation' in df.columns:\n",
    "            initial_len = len(df)\n",
    "            # Use .loc to avoid SettingWithCopyWarning\n",
    "            df.loc[:, 'n_citation'] = pd.to_numeric(df['n_citation'], errors='coerce')\n",
    "            \n",
    "            # This is the correct way to drop NaNs and ensure we have a new DataFrame\n",
    "            df = df.dropna(subset=['n_citation']).copy()\n",
    "            \n",
    "            # This assignment is now on a guaranteed copy, but .loc is still best practice.\n",
    "            df.loc[:, 'n_citation'] = df['n_citation'].astype(CONFIG['DTYPE_OPTIMIZATIONS']['n_citation'])\n",
    "            logger.info(f\"Removed {initial_len - len(df)} rows with missing citations\")\n",
    "        \n",
    "        # Handle year\n",
    "        if 'year' in df.columns:\n",
    "            # Use .loc to avoid SettingWithCopyWarning\n",
    "            df.loc[:, 'year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "            mode_year = int(df['year'].mode()[0]) if not df['year'].mode().empty else 2000\n",
    "            df.loc[:, 'year'] = df['year'].fillna(mode_year).astype(CONFIG['DTYPE_OPTIMIZATIONS']['year'])\n",
    "            logger.info(f\"Filled missing years with mode: {mode_year}\")\n",
    "        \n",
    "        # Process keywords in chunks to manage memory (already uses .loc, which is good)\n",
    "        if 'keywords' in df.columns:\n",
    "            logger.info(\"Processing keywords...\")\n",
    "            chunk_size = 5000\n",
    "\n",
    "            for i in range(0, len(df), chunk_size):\n",
    "                chunk_end = min(i + chunk_size, len(df))\n",
    "                df.loc[df.index[i:chunk_end], 'keywords'] = df['keywords'].iloc[i:chunk_end].apply(robust_keyword_parser)\n",
    "\n",
    "                if i % 20000 == 0:  # Progress update every 20k records\n",
    "                    logger.info(f\"Processed keywords for {chunk_end}/{len(df)} records\")\n",
    "                    check_memory_limit(CONFIG['MAX_MEMORY_MB'])\n",
    "\n",
    "            logger.info(\"Keywords processing completed\")\n",
    "            force_garbage_collection()\n",
    "\n",
    "        \n",
    "        # Clean text fields efficiently using .loc\n",
    "        text_cols = ['title', 'abstract', 'venue', 'doc_type']\n",
    "        for col in text_cols:\n",
    "            if col in df.columns:\n",
    "                df.loc[:, col] = df[col].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "                df.loc[:, col] = df[col].replace({'none': '', 'null': '', 'nan': ''})\n",
    "                logger.info(f\"Cleaned text column: {col}\")\n",
    "        \n",
    "        # Handle references using .loc\n",
    "        if 'references' in df.columns:\n",
    "            logger.info(\"Processing references...\")\n",
    "            def safe_reference_parser(ref):\n",
    "                if isinstance(ref, list):\n",
    "                    return ref\n",
    "                try:\n",
    "                    if isinstance(ref, str) and ref.startswith('['):\n",
    "                        return ast.literal_eval(ref)\n",
    "                except Exception:\n",
    "                    return []\n",
    "                return []\n",
    "            df.loc[:, 'references'] = df['references'].apply(safe_reference_parser)\n",
    "            logger.info(\"References processed\")\n",
    "        \n",
    "        # Memory optimization - convert to optimal dtypes using .loc\n",
    "        for col, dtype in CONFIG['DTYPE_OPTIMIZATIONS'].items():\n",
    "            if col in df.columns:\n",
    "                df.loc[:, col] = df[col].astype(dtype)\n",
    "        \n",
    "        logger.info(f\"Data cleaning completed. Shape: {df.shape}\")\n",
    "        logger.info(f\"Memory usage: {initial_memory:.1f}MB -> {get_memory_usage():.1f}MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data cleaning: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Clean data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df_clean = clean_data_efficiently(df)\n",
    "print(f\"Data cleaning completed. Final shape: {df_clean.shape}\")\n",
    "print(f\"Memory usage after cleaning: {get_memory_usage():.1f}MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6445a459-30fe-443c-963b-fbe33654f769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: dblp_cleaned.parquet\n",
      "Memory: 3959.8MB -> 3280.8MB (freed: 678.9MB)\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "cleaned_path = CONFIG['DATA_DIR'] / CONFIG['CLEANED_PARQUET_FILE']\n",
    "df_clean.to_parquet(cleaned_path, index=False)\n",
    "print(f\"Cleaned data saved to: {cleaned_path}\")\n",
    "\n",
    "# Free memory\n",
    "del df\n",
    "force_garbage_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c60c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:57:50,798 - INFO - [MEM: 3289.5MB] - Creating numerical features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CREATING NUMERICAL FEATURES\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:57:51,865 - INFO - [MEM: 3289.5MB] - Created title_length feature\n",
      "2025-08-28 03:57:52,808 - INFO - [MEM: 3289.5MB] - Created abstract_length feature\n",
      "2025-08-28 03:57:53,255 - INFO - [MEM: 3289.5MB] - Created num_keywords feature\n",
      "2025-08-28 03:57:53,279 - INFO - [MEM: 3289.5MB] - Created age feature\n",
      "2025-08-28 03:57:53,675 - INFO - [MEM: 3289.5MB] - Created num_references feature\n",
      "2025-08-28 03:57:54,604 - INFO - [MEM: 3289.5MB] - All numerical features created successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features created. Memory usage: 5618.5MB\n",
      "\n",
      "Numerical feature statistics:\n",
      "title_length: mean=70.25, std=27.32, max=900.0\n",
      "abstract_length: mean=708.48, std=572.34, max=72632.0\n",
      "num_keywords: mean=8.87, std=7.88, max=141.0\n",
      "age: mean=20.46, std=10.15, max=2025.0\n",
      "num_references: mean=7.44, std=10.37, max=1930.0\n",
      "has_doi: mean=0.82, std=0.38, max=1.0\n",
      "has_url: mean=1.00, std=0.06, max=1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 5: Memory-Efficient Feature Engineering - Numerical Features\n",
    "# ============================================================================\n",
    "\n",
    "def create_numerical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create numerical features efficiently\"\"\"\n",
    "    logger = setup_logging(\"numerical_features\")\n",
    "    logger.info(\"Creating numerical features\")\n",
    "    \n",
    "    try:\n",
    "        # Title length\n",
    "        if 'title' in df.columns:\n",
    "            df['title_length'] = df['title'].astype(str).str.len().astype(CONFIG['DTYPE_OPTIMIZATIONS']['title_length'])\n",
    "            logger.info(\"Created title_length feature\")\n",
    "        else:\n",
    "            df['title_length'] = 0\n",
    "            logger.warning(\"Title column not found, defaulting title_length to 0\")\n",
    "        \n",
    "        # Abstract length\n",
    "        if 'abstract' in df.columns:\n",
    "            df['abstract_length'] = df['abstract'].astype(str).str.len().astype(CONFIG['DTYPE_OPTIMIZATIONS']['abstract_length'])\n",
    "            logger.info(\"Created abstract_length feature\")\n",
    "        else:\n",
    "            df['abstract_length'] = 0\n",
    "            logger.warning(\"Abstract column not found, defaulting abstract_length to 0\")\n",
    "        \n",
    "        # Number of keywords\n",
    "        if 'keywords' in df.columns:\n",
    "            df['num_keywords'] = df['keywords'].apply(len).astype(CONFIG['DTYPE_OPTIMIZATIONS']['num_keywords'])\n",
    "            logger.info(\"Created num_keywords feature\")\n",
    "        else:\n",
    "            df['num_keywords'] = 0\n",
    "            logger.warning(\"Keywords column not found, defaulting num_keywords to 0\")\n",
    "        \n",
    "        # Age of publication\n",
    "        if 'year' in df.columns:\n",
    "            current_year = pd.Timestamp.now().year\n",
    "            df['age'] = (current_year - df['year']).clip(lower=0).astype(CONFIG['DTYPE_OPTIMIZATIONS']['age'])\n",
    "            logger.info(\"Created age feature\")\n",
    "        else:\n",
    "            df['age'] = 0\n",
    "            logger.warning(\"Year column not found, defaulting age to 0\")\n",
    "        \n",
    "        # Number of references\n",
    "        if 'references' in df.columns:\n",
    "            df['num_references'] = df['references'].apply(len).astype(CONFIG['DTYPE_OPTIMIZATIONS']['num_references'])\n",
    "            logger.info(\"Created num_references feature\")\n",
    "        else:\n",
    "            df['num_references'] = 0\n",
    "            logger.warning(\"References column not found, defaulting num_references to 0\")\n",
    "        \n",
    "        # Boolean features\n",
    "        df['has_doi'] = df['doi'].notna().astype(CONFIG['DTYPE_OPTIMIZATIONS']['has_doi']) if 'doi' in df.columns else 0\n",
    "        df['has_url'] = df['url'].notna().astype(CONFIG['DTYPE_OPTIMIZATIONS']['has_url']) if 'url' in df.columns else 0\n",
    "        \n",
    "        logger.info(\"All numerical features created successfully\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating numerical features: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Create numerical features\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CREATING NUMERICAL FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df_features = create_numerical_features(df_clean)\n",
    "print(f\"Numerical features created. Memory usage: {get_memory_usage():.1f}MB\")\n",
    "\n",
    "# Verify numerical features\n",
    "print(\"\\nNumerical feature statistics:\")\n",
    "for col in NUMERICAL_FEATURES_COLS:\n",
    "    if col in df_features.columns:\n",
    "        stats = df_features[col].describe()\n",
    "        print(f\"{col}: mean={stats['mean']:.2f}, std={stats['std']:.2f}, max={stats['max']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advanced-feature-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:57:54,816 - INFO - [MEM: 5581.2MB] - Creating advanced features (venue quality, author count)\n",
      "2025-08-28 03:57:54,817 - INFO - [MEM: 5581.2MB] - Calculating venue quality scores...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CREATING ADVANCED FEATURES\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:57:55,664 - INFO - [MEM: 5581.2MB] - Venue quality score created.\n",
      "2025-08-28 03:57:55,665 - INFO - [MEM: 5581.2MB] - Calculating number of authors...\n",
      "2025-08-28 03:57:59,256 - INFO - [MEM: 5581.2MB] - Number of authors feature created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced features created. Memory usage: 2022.2MB\n",
      "\n",
      "Updated list of numerical features:\n",
      "['title_length', 'abstract_length', 'num_keywords', 'age', 'num_references', 'has_doi', 'has_url', 'venue_quality_score', 'num_authors']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BLOCK 5.5: Advanced Feature Engineering (Venue & Author)\n",
    "# ============================================================================\n",
    "\n",
    "def create_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create more powerful features from venue and authors.\"\"\"\n",
    "    logger = setup_logging(\"advanced_features\")\n",
    "    logger.info(\"Creating advanced features (venue quality, author count)\")\n",
    "\n",
    "    try:\n",
    "        # 1. Venue Quality Score (based on mean citations)\n",
    "        if 'venue' in df.columns and 'n_citation' in df.columns:\n",
    "            logger.info(\"Calculating venue quality scores...\")\n",
    "            # Calculate mean citation per venue\n",
    "            venue_quality = df.groupby('venue')['n_citation'].mean().to_dict()\n",
    "            \n",
    "            # Map this back to the dataframe\n",
    "            df['venue_quality_score'] = df['venue'].map(venue_quality)\n",
    "            \n",
    "            # Fill missing venues with the global median citation count\n",
    "            global_median_citation = df['n_citation'].median()\n",
    "            df['venue_quality_score'].fillna(global_median_citation, inplace=True)\n",
    "            \n",
    "            df['venue_quality_score'] = df['venue_quality_score'].astype('float32')\n",
    "            logger.info(\"Venue quality score created.\")\n",
    "        else:\n",
    "            logger.warning(\"Skipping venue quality score (venue or n_citation column missing).\")\n",
    "            df['venue_quality_score'] = 0.0\n",
    "\n",
    "        # 2. Number of Authors\n",
    "        if 'authors' in df.columns:\n",
    "            logger.info(\"Calculating number of authors...\")\n",
    "            # The 'authors' column is a string representation of a list\n",
    "            def count_authors(authors_str):\n",
    "                if pd.isna(authors_str) or not isinstance(authors_str, str) or not authors_str.strip():\n",
    "                    return 1 # Default to 1 author if data is missing/malformed\n",
    "                try:\n",
    "                    # Use a simple comma split as a robust fallback\n",
    "                    return len(authors_str.split(','))\n",
    "                except:\n",
    "                    return 1\n",
    "            \n",
    "            df['num_authors'] = df['authors'].apply(count_authors).astype('int16')\n",
    "            logger.info(\"Number of authors feature created.\")\n",
    "        else:\n",
    "            logger.warning(\"Skipping author count (authors column missing).\")\n",
    "            df['num_authors'] = 1\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating advanced features: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Add the new features\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CREATING ADVANCED FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df_features = create_advanced_features(df_features)\n",
    "\n",
    "# Add new features to the list of numerical columns for the model\n",
    "NEW_NUMERICAL_COLS = ['venue_quality_score', 'num_authors']\n",
    "for col in NEW_NUMERICAL_COLS:\n",
    "    if col not in NUMERICAL_FEATURES_COLS:\n",
    "        NUMERICAL_FEATURES_COLS.append(col)\n",
    "\n",
    "print(f\"Advanced features created. Memory usage: {get_memory_usage():.1f}MB\")\n",
    "print(\"\\nUpdated list of numerical features:\")\n",
    "print(NUMERICAL_FEATURES_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a2e76dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:58:00,217 - INFO - [MEM: 2904.1MB] - Starting TF-IDF feature creation\n",
      "2025-08-28 03:58:00,217 - INFO - [MEM: 2904.1MB] - Processing TF-IDF for title...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CREATING TF-IDF FEATURES\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 03:58:34,140 - INFO - [MEM: 2904.1MB] - title TF-IDF shape: (1732752, 5000), sparsity: 0.9987\n",
      "2025-08-28 03:58:34,142 - INFO - [MEM: 2904.1MB] - Processing TF-IDF for abstract...\n",
      "2025-08-28 04:04:17,387 - INFO - [MEM: 2904.1MB] - abstract TF-IDF shape: (1732752, 5000), sparsity: 0.9917\n",
      "2025-08-28 04:04:17,390 - INFO - [MEM: 2904.1MB] - Processing TF-IDF for keywords...\n",
      "2025-08-28 04:04:40,805 - INFO - [MEM: 2904.1MB] - keywords TF-IDF shape: (1732752, 2500), sparsity: 0.9953\n",
      "2025-08-28 04:04:40,806 - INFO - [MEM: 2904.1MB] - Combining sparse features...\n",
      "2025-08-28 04:04:41,356 - INFO - [MEM: 2904.1MB] - Combined TF-IDF features shape: (1732752, 12500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features created. Shape: (1732752, 12500)\n",
      "Memory usage: 4307.3MB\n",
      "Saving TF-IDF features and vectorizers...\n",
      "TF-IDF features saved to: tfidf_combined_sparse_features.npz\n",
      "Vectorizers saved to: tfidf_vectorizers.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 6: Memory-Efficient TF-IDF Feature Engineering\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, coo_matrix, save_npz, csr_matrix\n",
    "import joblib\n",
    "\n",
    "def create_tfidf_features_efficiently(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Create TF-IDF features with memory optimization\"\"\"\n",
    "    logger = setup_logging(\"tfidf_features\")\n",
    "    logger.info(\"Starting TF-IDF feature creation\")\n",
    "    \n",
    "    vectorizers = {}\n",
    "    sparse_features = []\n",
    "    \n",
    "    try:\n",
    "        # Process each text field separately to manage memory\n",
    "        text_fields = ['title', 'abstract', 'keywords']\n",
    "        \n",
    "        for field in text_fields:\n",
    "            logger.info(f\"Processing TF-IDF for {field}...\")\n",
    "            \n",
    "            if field not in df.columns:\n",
    "                logger.warning(f\"{field} column not found, skipping\")\n",
    "                sparse_features.append(coo_matrix((len(df), 0), dtype=np.float32))\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare text data\n",
    "                if field == 'keywords':\n",
    "                    text_data = df[field].apply(\n",
    "                        lambda x: ' '.join(x) if isinstance(x, list) else str(x)\n",
    "                    ).fillna('')\n",
    "                else:\n",
    "                    text_data = df[field].astype(str).fillna('')\n",
    "                \n",
    "                # Check for empty documents\n",
    "                non_empty_mask = text_data.str.strip().astype(bool)\n",
    "                non_empty_count = non_empty_mask.sum()\n",
    "                \n",
    "                if non_empty_count == 0:\n",
    "                    logger.warning(f\"All {field} documents are empty\")\n",
    "                    sparse_features.append(coo_matrix((len(df), 0), dtype=np.float32))\n",
    "                    continue\n",
    "                \n",
    "                # Configure vectorizer based on field\n",
    "                if field == 'keywords':\n",
    "                    vectorizer = TfidfVectorizer(\n",
    "                        max_features=CONFIG['TFIDF_MAX_FEATURES'] // 2,  # Smaller for keywords\n",
    "                        min_df=CONFIG['TFIDF_MIN_DF_KEYWORDS'],\n",
    "                        ngram_range=(1, 1),  # Only unigrams for keywords\n",
    "                        stop_words=None,\n",
    "                        token_pattern=r'\\S+',\n",
    "                        dtype=np.float32  # Use float32 to save memory\n",
    "                    )\n",
    "                else:\n",
    "                    vectorizer = TfidfVectorizer(\n",
    "                        max_features=CONFIG['TFIDF_MAX_FEATURES'],\n",
    "                        min_df=CONFIG['TFIDF_MIN_DF_TITLE_ABSTRACT'],\n",
    "                        ngram_range=CONFIG['TFIDF_NGRAM_RANGE'],\n",
    "                        stop_words='english',\n",
    "                        dtype=np.float32\n",
    "                    )\n",
    "                \n",
    "                # Fit and transform\n",
    "                tfidf_matrix = vectorizer.fit_transform(text_data)\n",
    "                \n",
    "                # Convert to CSR for efficiency\n",
    "                if not isinstance(tfidf_matrix, csr_matrix):\n",
    "                    tfidf_matrix = tfidf_matrix.tocsr()\n",
    "                \n",
    "                sparse_features.append(tfidf_matrix)\n",
    "                vectorizers[field] = vectorizer\n",
    "                \n",
    "                logger.info(f\"{field} TF-IDF shape: {tfidf_matrix.shape}, \"\n",
    "                           f\"sparsity: {1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f}\")\n",
    "                \n",
    "                # Clean up temporary variables\n",
    "                del text_data, tfidf_matrix\n",
    "                check_memory_limit(CONFIG['MAX_MEMORY_MB'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {field}: {e}\")\n",
    "                sparse_features.append(coo_matrix((len(df), 0), dtype=np.float32))\n",
    "        \n",
    "        # Combine sparse features efficiently\n",
    "        logger.info(\"Combining sparse features...\")\n",
    "        if sparse_features:\n",
    "            combined_features = hstack(sparse_features, format='csr', dtype=np.float32)\n",
    "            logger.info(f\"Combined TF-IDF features shape: {combined_features.shape}\")\n",
    "        else:\n",
    "            combined_features = coo_matrix((len(df), 0), dtype=np.float32)\n",
    "            logger.warning(\"No TF-IDF features generated\")\n",
    "        \n",
    "        return combined_features, vectorizers\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in TF-IDF feature creation: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Create TF-IDF features\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CREATING TF-IDF FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "combined_tfidf, vectorizers = create_tfidf_features_efficiently(df_features)\n",
    "print(f\"TF-IDF features created. Shape: {combined_tfidf.shape}\")\n",
    "print(f\"Memory usage: {get_memory_usage():.1f}MB\")\n",
    "\n",
    "# Save TF-IDF features and vectorizers for the next steps\n",
    "print(\"Saving TF-IDF features and vectorizers...\")\n",
    "save_npz(CONFIG['TFIDF_COMBINED_FEATURES_PATH'], combined_tfidf)\n",
    "print(f\"TF-IDF features saved to: {CONFIG['TFIDF_COMBINED_FEATURES_PATH']}\")\n",
    "joblib.dump(vectorizers, CONFIG['VECTORIZERS_PATH'])\n",
    "print(f\"Vectorizers saved to: {CONFIG['VECTORIZERS_PATH']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lda-feature-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 04:05:35,450 - INFO - [MEM: 6380.3MB] - Starting LDA feature creation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CREATING TOPIC MODELING (LDA) FEATURES\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 04:05:35,956 - INFO - [MEM: 6380.3MB] - Creating document-term matrix with CountVectorizer for LDA...\n",
      "2025-08-28 04:06:47,530 - INFO - [MEM: 6380.3MB] - Document-term matrix shape for LDA: (1732752, 5000)\n",
      "2025-08-28 04:06:47,531 - INFO - [MEM: 6380.3MB] - Fitting LDA model with 10 topics...\n",
      "2025-08-28 04:46:00,947 - INFO - [MEM: 6380.3MB] - LDA topic distributions created. Shape: (1732752, 10)\n",
      "2025-08-28 04:46:00,990 - INFO - [MEM: 6380.3MB] - Saving LDA model and CountVectorizer...\n",
      "2025-08-28 04:46:01,039 - INFO - [MEM: 6380.3MB] - LDA model saved to: lda_model.pkl\n",
      "2025-08-28 04:46:01,040 - INFO - [MEM: 6380.3MB] - LDA CountVectorizer saved to: lda_count_vectorizer.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topic features created. New dataframe shape: (1732752, 38)\n",
      "Memory usage: 3066.7MB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BLOCK 6.5: Topic Modeling (LDA) Features\n",
    "# ============================================================================\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_lda_features(df: pd.DataFrame, config: Dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Create topic modeling features using Latent Dirichlet Allocation (LDA).\n",
    "    Includes logging, error handling, and saves the trained models.\n",
    "    \"\"\"\n",
    "    logger = setup_logging(\"lda_features\")\n",
    "    logger.info(\"Starting LDA feature creation\")\n",
    "\n",
    "    try:\n",
    "        if 'abstract' not in df.columns or df['abstract'].str.strip().eq('').all():\n",
    "            logger.warning(\"Abstract column is missing or all entries are empty. Skipping LDA.\")\n",
    "            return pd.DataFrame(index=df.index), []\n",
    "\n",
    "        # 1. Create a document-term matrix using CountVectorizer\n",
    "        logger.info(\"Creating document-term matrix with CountVectorizer for LDA...\")\n",
    "        text_data = df['abstract'].astype(str).fillna('')\n",
    "\n",
    "        lda_vectorizer = CountVectorizer(\n",
    "            max_df=config['LDA_MAX_DF'],\n",
    "            min_df=config['LDA_MIN_DF'],\n",
    "            max_features=config['LDA_MAX_FEATURES'],\n",
    "            stop_words='english',\n",
    "            dtype=np.int32\n",
    "        )\n",
    "\n",
    "        doc_term_matrix = lda_vectorizer.fit_transform(text_data)\n",
    "        logger.info(f\"Document-term matrix shape for LDA: {doc_term_matrix.shape}\")\n",
    "\n",
    "        # 2. Fit the LDA model\n",
    "        n_topics = config['LDA_N_TOPICS']\n",
    "        logger.info(f\"Fitting LDA model with {n_topics} topics...\")\n",
    "        \n",
    "        lda_model = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=config['RANDOM_STATE'],\n",
    "            n_jobs=config['N_JOBS'],\n",
    "            learning_method='online', # More memory efficient\n",
    "            batch_size=128,\n",
    "            verbose=0 # Keep console clean, rely on our logs\n",
    "        )\n",
    "        \n",
    "        topic_distributions = lda_model.fit_transform(doc_term_matrix)\n",
    "        logger.info(f\"LDA topic distributions created. Shape: {topic_distributions.shape}\")\n",
    "\n",
    "        # 3. Create topic feature columns\n",
    "        topic_columns = [f'topic_{i}' for i in range(n_topics)]\n",
    "        df_topics = pd.DataFrame(topic_distributions, columns=topic_columns, index=df.index)\n",
    "        for col in topic_columns:\n",
    "            df_topics[col] = df_topics[col].astype(np.float32)\n",
    "\n",
    "        # 4. Save the LDA model and its vectorizer\n",
    "        logger.info(\"Saving LDA model and CountVectorizer...\")\n",
    "        joblib.dump(lda_model, config['LDA_MODEL_PATH'])\n",
    "        joblib.dump(lda_vectorizer, config['LDA_VECTORIZER_PATH'])\n",
    "        logger.info(f\"LDA model saved to: {config['LDA_MODEL_PATH']}\")\n",
    "        logger.info(f\"LDA CountVectorizer saved to: {config['LDA_VECTORIZER_PATH']}\")\n",
    "\n",
    "        return df_topics, topic_columns\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error during LDA feature creation: {e}\", exc_info=True)\n",
    "        return pd.DataFrame(index=df.index), []\n",
    "\n",
    "# --- Add LDA features to the main dataframe ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CREATING TOPIC MODELING (LDA) FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df_topic_features, topic_cols = create_lda_features(df_features, CONFIG)\n",
    "\n",
    "if not df_topic_features.empty:\n",
    "    df_features = df_features.join(df_topic_features)\n",
    "    for col in topic_cols:\n",
    "        if col not in NUMERICAL_FEATURES_COLS:\n",
    "            NUMERICAL_FEATURES_COLS.append(col)\n",
    "    print(f\"LDA topic features created. New dataframe shape: {df_features.shape}\")\n",
    "    print(f\"Memory usage: {get_memory_usage():.1f}MB\")\n",
    "else:\n",
    "    print(\"LDA feature creation was skipped or failed. Continuing without topic features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "028616a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 04:46:10,011 - INFO - [MEM: 3083.2MB] - Finalizing dataset\n",
      "2025-08-28 04:46:10,028 - INFO - [MEM: 3083.2MB] - Performing final data quality checks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINALIZING DATASET\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 04:46:28,450 - INFO - [MEM: 3083.2MB] - Converted lang to category dtype\n",
      "2025-08-28 04:46:28,799 - INFO - [MEM: 3083.2MB] - Converted venue to category dtype\n",
      "2025-08-28 04:46:33,327 - INFO - [MEM: 3083.2MB] - Converted doc_type to category dtype\n",
      "2025-08-28 04:46:38,924 - INFO - [MEM: 3083.2MB] - Memory optimization: 4459.8MB -> 4152.6MB\n",
      "2025-08-28 04:46:38,929 - INFO - [MEM: 3083.2MB] - Final dataset shape: (1732752, 38)\n",
      "2025-08-28 04:46:38,939 - INFO - [MEM: 3083.2MB] - Target variable (n_citation) range: 0.0 - 121860.0\n",
      "2025-08-28 04:46:38,940 - INFO - [MEM: 3083.2MB] - Available features: ['id', 'title', 'doi', 'issue', 'keywords', 'lang', 'venue', 'year', 'n_citation', 'page_start', 'page_end', 'volume', 'issn', 'isbn', 'url', 'abstract', 'authors', 'doc_type', 'references', 'title_length', 'abstract_length', 'num_keywords', 'age', 'num_references', 'has_doi', 'has_url', 'venue_quality_score', 'num_authors', 'topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset finalized. Shape: (1732752, 38)\n",
      "Memory usage: 6621.0MB\n",
      "Final processed dataset saved to: dblp_final_processed.parquet\n",
      "Numerical features summary saved to: numerical_features_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 7: Final Data Preparation and Saving\n",
    "# ============================================================================\n",
    "\n",
    "def finalize_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Final dataset preparation with memory optimization\"\"\"\n",
    "    logger = setup_logging(\"finalize_dataset\")\n",
    "    logger.info(\"Finalizing dataset\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure all numerical features exist with correct dtypes\n",
    "        for col in NUMERICAL_FEATURES_COLS:\n",
    "            if col in df.columns:\n",
    "                if col in CONFIG['DTYPE_OPTIMIZATIONS']:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "                    df[col] = df[col].astype(CONFIG['DTYPE_OPTIMIZATIONS'][col])\n",
    "            else:\n",
    "                # Add missing feature with default value\n",
    "                default_dtype = CONFIG['DTYPE_OPTIMIZATIONS'].get(col, 'float32')\n",
    "                df[col] = pd.Series(0, index=df.index, dtype=default_dtype)\n",
    "                logger.warning(f\"Added missing feature {col} with default value 0\")\n",
    "        \n",
    "        # Final data quality checks\n",
    "        logger.info(\"Performing final data quality checks...\")\n",
    "        \n",
    "        # Check for infinite values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        inf_counts = {}\n",
    "        for col in numeric_cols:\n",
    "            inf_count = np.isinf(df[col]).sum()\n",
    "            if inf_count > 0:\n",
    "                df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "                inf_counts[col] = inf_count\n",
    "        \n",
    "        if inf_counts:\n",
    "            logger.warning(f\"Replaced infinite values in columns: {inf_counts}\")\n",
    "        \n",
    "        # Final memory optimization\n",
    "        memory_before = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        # Optimize string columns\n",
    "        string_cols = df.select_dtypes(include=['object']).columns\n",
    "        for col in string_cols:\n",
    "\n",
    "        # Optimize string columns\n",
    "            string_cols = df.select_dtypes(include=['object']).columns\n",
    "            for col in string_cols:\n",
    "                if col in ['title', 'abstract', 'venue', 'doc_type', 'lang']:\n",
    "                    unique_ratio = df[col].nunique() / len(df)\n",
    "                    if unique_ratio < 0.1:  # Less than 10% unique values\n",
    "                        df[col] = df[col].astype('category')\n",
    "                        logger.info(f\"Converted {col} to category dtype\")\n",
    "            \n",
    "            memory_after = df.memory_usage(deep=True).sum() / 1024**2\n",
    "            logger.info(f\"Memory optimization: {memory_before:.1f}MB -> {memory_after:.1f}MB\")\n",
    "            \n",
    "            # Final dataset summary\n",
    "            logger.info(f\"Final dataset shape: {df.shape}\")\n",
    "            logger.info(f\"Target variable (n_citation) range: {df['n_citation'].min()} - {df['n_citation'].max()}\")\n",
    "            logger.info(f\"Available features: {list(df.columns)}\")\n",
    "            \n",
    "            return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error finalizing dataset: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Finalize the dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINALIZING DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df_final = finalize_dataset(df_features)\n",
    "print(f\"Dataset finalized. Shape: {df_final.shape}\")\n",
    "print(f\"Memory usage: {get_memory_usage():.1f}MB\")\n",
    "\n",
    "# Save the final processed dataset\n",
    "final_processed_path = CONFIG['DATA_DIR'] / 'dblp_final_processed.parquet'\n",
    "df_final.to_parquet(final_processed_path, index=False)\n",
    "print(f\"Final processed dataset saved to: {final_processed_path}\")\n",
    "\n",
    "# Create a summary of numerical features for later use\n",
    "numerical_summary = df_final[NUMERICAL_FEATURES_COLS].describe()\n",
    "summary_path = CONFIG['DATA_DIR'] / 'numerical_features_summary.csv'\n",
    "numerical_summary.to_csv(summary_path)\n",
    "print(f\"Numerical features summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916e34dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 04:47:05,401 - INFO - [MEM: 3728.1MB] - Creating train-test split with test_size=0.2\n",
      "2025-08-28 04:47:05,414 - INFO - [MEM: 3728.1MB] - Target (log1p) statistics: min=0.0, max=11.710636329907006, mean=2.20\n",
      "2025-08-28 04:47:05,536 - INFO - [MEM: 3728.1MB] - Numerical features shape: (1732752, 19)\n",
      "2025-08-28 04:47:05,537 - INFO - [MEM: 3728.1MB] - Loading TF-IDF features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CREATING TRAIN-TEST SPLIT\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 04:47:07,821 - INFO - [MEM: 3728.1MB] - TF-IDF features shape: (1732752, 12500)\n",
      "2025-08-28 04:47:07,822 - INFO - [MEM: 3728.1MB] - Combining numerical and TF-IDF features...\n",
      "2025-08-28 04:47:09,814 - INFO - [MEM: 3728.1MB] - Combined features shape: (1732752, 12519)\n",
      "2025-08-28 04:47:09,815 - INFO - [MEM: 3728.1MB] - Combined features sparsity: 0.9938\n",
      "2025-08-28 04:47:11,174 - INFO - [MEM: 3728.1MB] - Train set shape: X=(1386201, 12519), y=(1386201,)\n",
      "2025-08-28 04:47:11,177 - INFO - [MEM: 3728.1MB] - Test set shape: X=(346551, 12519), y=(346551,)\n",
      "2025-08-28 04:47:11,178 - INFO - [MEM: 3728.1MB] - Train target range: 0.0-11.710636329907006\n",
      "2025-08-28 04:47:11,178 - INFO - [MEM: 3728.1MB] - Test target range: 0.0-11.233317376674892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split completed. Memory usage: 1203.7MB\n",
      "Split indices saved to: train_test_indices.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 8: Memory-Efficient Train-Test Split\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "def create_train_test_split_efficiently(df: pd.DataFrame, test_size: float = 0.2) -> tuple:\n",
    "    \"\"\"Create train-test split with memory efficiency\"\"\"\n",
    "    logger = setup_logging(\"train_test_split\")\n",
    "    logger.info(f\"Creating train-test split with test_size={test_size}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare target variable\n",
    "        # Prepare target variable with log1p transform (for better regression)\n",
    "        y = np.log1p(df['n_citation'].values)\n",
    "        logger.info(f\"Target (log1p) statistics: min={y.min()}, max={y.max()}, mean={y.mean():.2f}\")\n",
    "\n",
    "        \n",
    "        # Prepare numerical features\n",
    "        X_numerical = df[NUMERICAL_FEATURES_COLS].values.astype(np.float32)\n",
    "        logger.info(f\"Numerical features shape: {X_numerical.shape}\")\n",
    "        \n",
    "        # Load TF-IDF features\n",
    "        logger.info(\"Loading TF-IDF features...\")\n",
    "        X_tfidf = load_npz(CONFIG['TFIDF_COMBINED_FEATURES_PATH'])\n",
    "        logger.info(f\"TF-IDF features shape: {X_tfidf.shape}\")\n",
    "        \n",
    "        # Combine features\n",
    "        logger.info(\"Combining numerical and TF-IDF features...\")\n",
    "        X_numerical_sparse = csr_matrix(X_numerical, dtype=np.float32)\n",
    "        X_combined = hstack([X_numerical_sparse, X_tfidf], format='csr', dtype=np.float32)\n",
    "        \n",
    "        logger.info(f\"Combined features shape: {X_combined.shape}\")\n",
    "        logger.info(f\"Combined features sparsity: {1 - X_combined.nnz / (X_combined.shape[0] * X_combined.shape[1]):.4f}\")\n",
    "        \n",
    "        # Stratified split for better distribution\n",
    "        # Create stratification bins for continuous target\n",
    "        y_bins = pd.qcut(y, q=min(10, len(np.unique(y))), duplicates='drop', labels=False)\n",
    "        \n",
    "        # Perform split\n",
    "        indices = np.arange(len(y))\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            indices, \n",
    "            test_size=test_size, \n",
    "            stratify=y_bins,\n",
    "            random_state=CONFIG['RANDOM_STATE']\n",
    "        )\n",
    "        \n",
    "        # Create splits\n",
    "        X_train = X_combined[train_idx]\n",
    "        X_test = X_combined[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "        \n",
    "        logger.info(f\"Train set shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "        logger.info(f\"Test set shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "        logger.info(f\"Train target range: {y_train.min()}-{y_train.max()}\")\n",
    "        logger.info(f\"Test target range: {y_test.min()}-{y_test.max()}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, train_idx, test_idx\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating train-test split: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Create train-test split\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CREATING TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_idx, test_idx = create_train_test_split_efficiently(df_final)\n",
    "print(f\"Train-test split completed. Memory usage: {get_memory_usage():.1f}MB\")\n",
    "\n",
    "# Save split indices for reproducibility\n",
    "split_indices = {\n",
    "    'train_idx': train_idx,\n",
    "    'test_idx': test_idx\n",
    "}\n",
    "split_indices_path = CONFIG['DATA_DIR'] / 'train_test_indices.pkl'\n",
    "joblib.dump(split_indices, split_indices_path)\n",
    "print(f\"Split indices saved to: {split_indices_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ridge-regression-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Alternative Model: Ridge Regression ---\n",
      "Training RidgeCV on the full dataset...\n",
      "Best alpha found by RidgeCV: 1.0\n",
      "\n",
      "Evaluating the Ridge model on the test set...\n",
      "WARNING: 44 Ridge predictions overflowed. Capping them.\n",
      "\n",
      "--- Ridge Model Evaluation ---\n",
      "Test R² (on original citation scale): -53.2043\n",
      "Test RMSE (on original citation scale): 2413.52\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BLOCK 9A: Alternative Model - Ridge Regression\n",
    "# ============================================================================\n",
    "# from sklearn.linear_model import RidgeCV\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import numpy as np\n",
    "\n",
    "# print(\"--- Training Alternative Model: Ridge Regression ---\")\n",
    "\n",
    "# # 1. Define the model with built-in cross-validation to find the best alpha\n",
    "# # RidgeCV is very efficient for finding the regularization strength.\n",
    "# alphas_to_try = [0.1, 1.0, 10.0, 100.0]\n",
    "# ridge_cv_model = RidgeCV(alphas=alphas_to_try, scoring='neg_root_mean_squared_error', cv=3)\n",
    "\n",
    "# # 2. Train the model on the full training data (it's fast!)\n",
    "# # y_train is already log-transformed from the previous step.\n",
    "# print(\"Training RidgeCV on the full dataset...\")\n",
    "# ridge_cv_model.fit(X_train, y_train)\n",
    "\n",
    "# print(f\"Best alpha found by RidgeCV: {ridge_cv_model.alpha_}\")\n",
    "\n",
    "# # 3. Predict and evaluate\n",
    "# print(\"\\nEvaluating the Ridge model on the test set...\")\n",
    "# y_pred_log_ridge = ridge_cv_model.predict(X_test)\n",
    "# y_test_original_scale = np.expm1(y_test) # Recreate for clarity\n",
    "\n",
    "# # --- FIX for potential overflow ---\n",
    "# # Cap large log-scale predictions to prevent overflow with np.expm1\n",
    "# log_pred_cap = np.log1p(y_test_original_scale.max()) + 1 # Cap 1 log unit above max known value\n",
    "# overflow_mask_ridge = y_pred_log_ridge > log_pred_cap\n",
    "# if overflow_mask_ridge.any():\n",
    "#     print(f\"WARNING: {overflow_mask_ridge.sum()} Ridge predictions overflowed. Capping them.\")\n",
    "#     y_pred_log_ridge[overflow_mask_ridge] = log_pred_cap\n",
    "\n",
    "# # Inverse transform to compare on the original citation scale\n",
    "# y_pred_original_scale_ridge = np.expm1(y_pred_log_ridge)\n",
    "\n",
    "# # Ensure predictions are non-negative\n",
    "# y_pred_original_scale_ridge[y_pred_original_scale_ridge < 0] = 0\n",
    "\n",
    "# # Calculate metrics\n",
    "# r2_ridge = r2_score(y_test_original_scale, y_pred_original_scale_ridge)\n",
    "# rmse_ridge = mean_squared_error(y_test_original_scale, y_pred_original_scale_ridge, squared=False)\n",
    "\n",
    "# print(\"\\n--- Ridge Model Evaluation ---\")\n",
    "# print(f\"Test R² (on original citation scale): {r2_ridge:.4f}\")\n",
    "# print(f\"Test RMSE (on original citation scale): {rmse_ridge:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1818bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a sample of 100000 for hyperparameter search.\n",
      "\n",
      "Starting hyperparameter search on the data sample...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.245616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 913223\n",
      "[LightGBM] [Info] Number of data points in the train set: 66666, number of used features: 11573\n",
      "[LightGBM] [Info] Start training from score 2.079442\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BLOCK 9: Efficient Model Training with Hyperparameter Tuning\n",
    "# ============================================================================\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# --- STRATEGY: Tune on a sample for speed, then train best model on full data ---\n",
    "\n",
    "# 1. Create a smaller sample for fast hyperparameter tuning\n",
    "sample_size = 100000  # Use 100k records for the search\n",
    "if len(y_train) > sample_size:\n",
    "    sample_indices = np.random.choice(X_train.shape[0], sample_size, replace=False)\n",
    "    X_train_sample = X_train[sample_indices]\n",
    "    y_train_sample = y_train[sample_indices]\n",
    "else:\n",
    "    X_train_sample = X_train\n",
    "    y_train_sample = y_train\n",
    "\n",
    "print(f\"Using a sample of {X_train_sample.shape[0]} for hyperparameter search.\")\n",
    "\n",
    "# 2. Define a more focused hyperparameter grid\n",
    "param_dist = {\n",
   "     'num_leaves': [31, 50, 70], # Slightly reduced to keep search space manageable\n",
    "    'max_depth': [-1, 15, 25],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [200, 400, 600],  # Reduced for faster search\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
   "     # FIX: Add min_child_samples to the search. This is crucial for preventing\n",
   "     # the \"no further splits\" warning by allowing smaller, more specialized leaf nodes.\n",
   "     'min_child_samples': [5, 10, 20],\n",
    "    'reg_alpha': [0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "# 3. Setup and run RandomizedSearchCV on the SAMPLE data\n",
    "lgb_model = LGBMRegressor(objective='regression_l1', random_state=42, n_jobs=-1)\n",
    "search = RandomizedSearchCV(\n",
    "    lgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,  # Reduced iterations for speed\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error', # More stable for regression\n",
    "    verbose=1,\n",
    "    n_jobs=1  # LGBM is already multi-threaded\n",
    ")\n",
    "\n",
    "print(\"\\nStarting hyperparameter search on the data sample...\")\n",
    "# BUG FIX: y_train is already log-transformed from the previous step.\n",
    "# We use y_train_sample here.\n",
    "search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f\"\\nBest Params found: {search.best_params_}\")\n",
    "\n",
    "# 4. Train the final, best model on the FULL training data\n",
    "print(\"\\nTraining the best model on the full dataset...\")\n",
    "best_model = LGBMRegressor(objective='regression_l1', random_state=42, n_jobs=-1, **search.best_params_)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict and evaluate\n",
    "print(\"\\nEvaluating the final model on the test set...\")\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_test_original_scale = np.expm1(y_test)\n",
    "\n",
    "# --- FIX for potential overflow ---\n",
    "# Cap large log-scale predictions to prevent overflow with np.expm1\n",
    "log_pred_cap = np.log1p(y_test_original_scale.max()) + 1 # Cap 1 log unit above max known value\n",
    "overflow_mask_lgbm = y_pred_log > log_pred_cap\n",
    "if overflow_mask_lgbm.any():\n",
    "    print(f\"WARNING: {overflow_mask_lgbm.sum()} LightGBM predictions overflowed. Capping them.\")\n",
    "    y_pred_log[overflow_mask_lgbm] = log_pred_cap\n",
    "\n",
    "# Inverse transform prediction to compare on original scale\n",
    "y_pred_original_scale = np.expm1(y_pred_log)\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "y_pred_original_scale[y_pred_original_scale < 0] = 0\n",
    "\n",
    "r2 = r2_score(y_test_original_scale, y_pred_original_scale)\n",
    "rmse = mean_squared_error(y_test_original_scale, y_pred_original_scale, squared=False)\n",
    "\n",
    "print(\"\\n--- Final Model Evaluation ---\")\n",
    "print(f\"Test R² (on original citation scale): {r2:.4f}\")\n",
    "print(f\"Test RMSE (on original citation scale): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716748e-7274-44fb-bc15-cacb8f56a96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98da3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 10: Model Evaluation and Visualization\n",
    "# ============================================================================\n",
    "\n",
    "def create_evaluation_visualizations(models: Dict, results: Dict, y_test, X_test):\n",
    "    \"\"\"Create evaluation visualizations efficiently\"\"\"\n",
    "    logger = setup_logging(\"evaluation_viz\")\n",
    "    logger.info(\"Creating evaluation visualizations\")\n",
    "    \n",
    "    try:\n",
    "        # Set up the plotting style\n",
    "        plt.style.use('default')\n",
    "        fig_size = (15, 10)\n",
    "        \n",
    "        # Create subplots\n",
    "        n_models = len(models)\n",
    "        fig, axes = plt.subplots(2, n_models, figsize=fig_size)\n",
    "        if n_models == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "        \n",
    "        colors = ['blue', 'red', 'green', 'orange']\n",
    "        \n",
    "        for i, (model_name, model) in enumerate(models.items()):\n",
    "            logger.info(f\"Creating plots for {model_name}\")\n",
    "            \n",
    "            # Get predictions\n",
    "            if model_name == 'random_forest' and X_test.shape[1] > 10000:\n",
    "                logger.warning(f\"Skipping visualization for {model_name} due to memory constraints\")\n",
    "                continue\n",
    "                \n",
    "            if hasattr(model, 'predict'):\n",
    "                if model_name == 'random_forest' and X_test.shape[1] <= 10000:\n",
    "                    X_test_dense = X_test.toarray().astype(np.float32)\n",
    "                    y_pred = model.predict(X_test_dense)\n",
    "                    del X_test_dense\n",
    "                else:\n",
    "                    y_pred = model.predict(X_test)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            # Plot 1: Actual vs Predicted\n",
    "            axes[0, i].scatter(y_test, y_pred, alpha=0.5, color=color, s=1)\n",
    "            axes[0, i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "            axes[0, i].set_xlabel('Actual Citations')\n",
    "            axes[0, i].set_ylabel('Predicted Citations')\n",
    "            axes[0, i].set_title(f'{model_name.title()}: Actual vs Predicted')\n",
    "            axes[0, i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add R² score to the plot\n",
    "            r2 = results[model_name]['test_r2']\n",
    "            axes[0, i].text(0.05, 0.95, f'R² = {r2:.3f}', \n",
    "                          transform=axes[0, i].transAxes, \n",
    "                          verticalalignment='top',\n",
    "                          bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Plot 2: Residuals\n",
    "            residuals = y_test - y_pred\n",
    "            axes[1, i].scatter(y_pred, residuals, alpha=0.5, color=color, s=1)\n",
    "            axes[1, i].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "            axes[1, i].set_xlabel('Predicted Citations')\n",
    "            axes[1, i].set_ylabel('Residuals')\n",
    "            axes[1, i].set_title(f'{model_name.title()}: Residual Plot')\n",
    "            axes[1, i].grid(True, alpha=0.3)\n",
    "            \n",
    "            del y_pred, residuals\n",
    "            force_garbage_collection()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plots_dir = CONFIG['DATA_DIR'] / 'plots'\n",
    "        plots_dir.mkdir(exist_ok=True)\n",
    "        plot_path = plots_dir / 'model_evaluation.png'\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        logger.info(f\"Evaluation plots saved to: {plot_path}\")\n",
    "        \n",
    "        # Create feature importance plot (for models that support it)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for i, (model_name, model) in enumerate(models.items()):\n",
    "            if hasattr(model, 'feature_importances_') and model_name == 'random_forest':\n",
    "                # Get feature importances\n",
    "                importances = model.feature_importances_\n",
    "                \n",
    "                # Focus on numerical features (first 7 features)\n",
    "                numerical_importances = importances[:len(NUMERICAL_FEATURES_COLS)]\n",
    "                \n",
    "                plt.subplot(1, len(models), i+1)\n",
    "                plt.bar(range(len(NUMERICAL_FEATURES_COLS)), numerical_importances)\n",
    "                plt.xlabel('Feature Index')\n",
    "                plt.ylabel('Importance')\n",
    "                plt.title(f'{model_name.title()}: Feature Importance\\n(Numerical Features Only)')\n",
    "                plt.xticks(range(len(NUMERICAL_FEATURES_COLS)), \n",
    "                          [col[:10] for col in NUMERICAL_FEATURES_COLS], \n",
    "                          rotation=45)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        if any(hasattr(model, 'feature_importances_') for model in models.values()):\n",
    "            plt.tight_layout()\n",
    "            importance_plot_path = plots_dir / 'feature_importance.png'\n",
    "            plt.savefig(importance_plot_path, dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            logger.info(f\"Feature importance plot saved to: {importance_plot_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating visualizations: {e}\", exc_info=True)\n",
    "\n",
    "# Create evaluation visualizations\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CREATING EVALUATION VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "create_evaluation_visualizations(trained_models, model_results, y_test, X_test)\n",
    "print(f\"Visualizations completed. Memory usage: {get_memory_usage():.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286743ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BLOCK 11: Final Summary and Cleanup\n",
    "# ============================================================================\n",
    "\n",
    "def generate_final_summary(df: pd.DataFrame, models: Dict, results: Dict, config: Dict):\n",
    "    \"\"\"Generate final pipeline summary\"\"\"\n",
    "    logger = setup_logging(\"final_summary\")\n",
    "    logger.info(\"Generating final pipeline summary\")\n",
    "    \n",
    "    try:\n",
    "        summary = {\n",
    "            'pipeline_info': {\n",
    "                'total_records_processed': len(df),\n",
    "                'final_features': df.shape[1],\n",
    "                'numerical_features': len(NUMERICAL_FEATURES_COLS),\n",
    "                'tfidf_features_file': str(config['TFIDF_COMBINED_FEATURES_PATH']),\n",
    "                'memory_limit_mb': config['MAX_MEMORY_MB'],\n",
    "                'random_state': config['RANDOM_STATE']\n",
    "            },\n",
    "            'data_summary': {\n",
    "                'target_variable': 'n_citation',\n",
    "                'target_range': f\"{df['n_citation'].min()} - {df['n_citation'].max()}\",\n",
    "                'target_mean': float(df['n_citation'].mean()),\n",
    "                'target_median': float(df['n_citation'].median()),\n",
    "                'target_std': float(df['n_citation'].std())\n",
    "            },\n",
    "            'model_performance': {},\n",
    "            'feature_info': {\n",
    "                'numerical_features': NUMERICAL_FEATURES_COLS,\n",
    "                'text_fields_processed': ['title', 'abstract', 'keywords']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add model performance\n",
    "        for model_name, metrics in results.items():\n",
    "            summary['model_performance'][model_name] = {\n",
    "                'test_r2': float(metrics['test_r2']),\n",
    "                'test_rmse': float(np.sqrt(metrics['test_mse'])),\n",
    "                'test_mae': float(metrics['test_mae']),\n",
    "                'training_time_seconds': float(metrics['training_time'])\n",
    "            }\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = config['DATA_DIR'] / 'pipeline_summary.json'\n",
    "        import json\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Pipeline summary saved to: {summary_path}\")\n",
    "        \n",
    "        # Print summary to console\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"✓ Dataset processed: {summary['pipeline_info']['total_records_processed']:,} records\")\n",
    "        print(f\"✓ Features created: {summary['pipeline_info']['final_features']} total\")\n",
    "        print(f\"✓ Models trained: {len(models)}\")\n",
    "        print(f\"✓ Memory usage: {get_memory_usage():.1f}MB\")\n",
    "        \n",
    "        print(f\"\\nTarget Variable Statistics:\")\n",
    "        print(f\"  Range: {summary['data_summary']['target_range']}\")\n",
    "        print(f\"  Mean: {summary['data_summary']['target_mean']:.2f}\")\n",
    "        print(f\"  Median: {summary['data_summary']['target_median']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nBest Model Performance:\")\n",
    "        best_model = max(results.keys(), key=lambda k: results[k]['test_r2'])\n",
    "        best_r2 = results[best_model]['test_r2']\n",
    "        best_rmse = np.sqrt(results[best_model]['test_mse'])\n",
    "        print(f\"  Model: {best_model}\")\n",
    "        print(f\"  Test R²: {best_r2:.4f}\")\n",
    "        print(f\"  Test RMSE: {best_rmse:.2f}\")\n",
    "        \n",
    "        print(f\"\\nFiles Generated:\")\n",
    "        print(f\"  ✓ {config['CLEANED_PARQUET_FILE']}\")\n",
    "        print(f\"  ✓ {config['TFIDF_COMBINED_FEATURES_PATH'].name}\")\n",
    "        print(f\"  ✓ {config['VECTORIZERS_PATH'].name}\")\n",
    "        print(f\"  ✓ dblp_final_processed.parquet\")\n",
    "        print(f\"  ✓ Model files in models/ directory\")\n",
    "        print(f\"  ✓ Plots in plots/ directory\")\n",
    "        print(f\"  ✓ pipeline_summary.json\")\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating summary: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING FINAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_summary = generate_final_summary(df_final, trained_models, model_results, CONFIG)\n",
    "\n",
    "# Final cleanup\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL CLEANUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Clean up large variables\n",
    "del X_train, X_test, y_train, y_test\n",
    "del df_clean, df_features, df_final\n",
    "del combined_tfidf, vectorizers\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"Pipeline completed successfully!\")\n",
    "print(f\"Final memory usage: {get_memory_usage():.1f}MB\")\n",
    "print(f\"Total execution time logged in individual step logs.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEMORY-EFFICIENT DBLP CITATION NETWORK PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
